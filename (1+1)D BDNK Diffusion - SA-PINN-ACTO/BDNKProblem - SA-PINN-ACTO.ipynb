{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f2017-62bf-4a73-aed2-7f0ec605e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Device configuration and core PyTorch setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "DTYPE = torch.float32\n",
    "device = torch.device('cuda')\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.init()\n",
    "    torch.rand(1, device=device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from SA_PINN_ACTO import PINN_BDNK_1D\n",
    "from IC_1D import IC_BDNK\n",
    "\n",
    "# Hyperparameters for network architecture and training schedule\n",
    "Nl, Nn = 10, 70\n",
    "t_end = 10.0\n",
    "L = 50.0\n",
    "adam_epochs = 35_000\n",
    "lr_net = 5e-3\n",
    "lr_mask = 4e-2 \n",
    "\n",
    "# BDNK simulation configuration and background field setup\n",
    "from BDNK_Functions import *\n",
    "BDNK_simulation = 2\n",
    "setup_external_Tv(BDNK_simulation, L)\n",
    "\n",
    "# Sampling parameters and domain sampling\n",
    "N_colloc = 50000\n",
    "# In the ACTO case, the N_ic below are not collocation points where an IC residual will be computed,\n",
    "# but points at which the exact initial condition gets computed\n",
    "N_ic = 1000\n",
    "\n",
    "def lhs_box(n, low, high, rng=np.random):\n",
    "    low, high = np.asarray(low, float), np.asarray(high, float)\n",
    "    D = low.size\n",
    "    H = np.empty((n, D), float)\n",
    "    for j in range(D):\n",
    "        P = (rng.permutation(n) + rng.rand(n)) / n\n",
    "        H[:, j] = low[j] + P * (high[j] - low[j])\n",
    "    return H\n",
    "    \n",
    "X_colloc_np = lhs_box(N_colloc, low=np.array([0.0, -L]), high=np.array([t_end, L])).astype(np.float32)\n",
    "\n",
    "# Construction of initial condition sampling grid\n",
    "x_edges = np.linspace(-L, L, N_ic+1)\n",
    "x_ic = (0.5 * (x_edges[:-1] + x_edges[1:])).reshape(-1, 1)\n",
    "t_ic = np.zeros_like(x_ic)\n",
    "X_ic = np.hstack((t_ic, x_ic))\n",
    "\n",
    "X_ic_t = torch.tensor(X_ic, dtype=DTYPE, device=device)\n",
    "J0_ic_t, alpha_ic_t, _ = IC_BDNK(X_ic_t, L)\n",
    "\n",
    "# Scaling factors for numerical stability of IC enforcement\n",
    "with torch.no_grad():\n",
    "    sJ0 = J0_ic_t.abs().max().clamp_min(1e-12).item()\n",
    "    sA  = alpha_ic_t.abs().max().clamp_min(1e-12).item()\n",
    "print(f\"[scales] sJ0={sJ0:.3e}, sA={sA:.3e}\")\n",
    "\n",
    "# Sorting IC data to enable fast 1D interpolation\n",
    "x_ic_torch = X_ic_t[:, 1:2].contiguous().view(-1)\n",
    "x_sorted, idx_sort = torch.sort(x_ic_torch)\n",
    "J0_sorted    = J0_ic_t.view(-1)[idx_sort]\n",
    "alpha_sorted = alpha_ic_t.view(-1)[idx_sort]\n",
    "\n",
    "@torch.no_grad()\n",
    "def _torch_lin_interp_1d(xq: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    xq_flat = xq.view(-1)\n",
    "    xq_clamped = xq_flat.clamp(min=x[0], max=x[-1])\n",
    "    idx_hi = torch.searchsorted(x, xq_clamped, right=True)\n",
    "    idx_hi = idx_hi.clamp(min=1, max=x.numel() - 1)\n",
    "    idx_lo = idx_hi - 1\n",
    "    x0 = x[idx_lo]; x1 = x[idx_hi]\n",
    "    y0 = y[idx_lo]; y1 = y[idx_hi]\n",
    "    denom = (x1 - x0)\n",
    "    denom = torch.where(denom.abs() > 0, denom, torch.ones_like(denom))\n",
    "    w = (xq_clamped - x0) / denom\n",
    "    yq = y0 + w * (y1 - y0)\n",
    "    return yq.view_as(xq)\n",
    "\n",
    "# Initial condition functions passed to the neural network (physical scale)\n",
    "def J0_ic_func(x_phys: torch.Tensor) -> torch.Tensor:\n",
    "    if x_sorted.device != x_phys.device or x_sorted.dtype != x_phys.dtype:\n",
    "        xk = x_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\n",
    "        yk = J0_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\n",
    "    else:\n",
    "        xk, yk = x_sorted, J0_sorted\n",
    "    yq = _torch_lin_interp_1d(x_phys.view(-1), xk, yk)\n",
    "    return yq.view(-1, 1)\n",
    "\n",
    "def alpha_ic_func(x_phys: torch.Tensor) -> torch.Tensor:\n",
    "    if x_sorted.device != x_phys.device or x_sorted.dtype != x_phys.dtype:\n",
    "        xk = x_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\n",
    "        yk = alpha_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\n",
    "    else:\n",
    "        xk, yk = x_sorted, alpha_sorted\n",
    "    yq = _torch_lin_interp_1d(x_phys.view(-1), xk, yk)\n",
    "    return yq.view(-1, 1)\n",
    "\n",
    "# Scaled initial condition functions (used internally by the model)\n",
    "def J0_ic_func_scaled(x_phys: torch.Tensor) -> torch.Tensor:\n",
    "    return J0_ic_func(x_phys) / sJ0\n",
    "\n",
    "def alpha_ic_func_scaled(x_phys: torch.Tensor) -> torch.Tensor:\n",
    "    return alpha_ic_func(x_phys) / sA\n",
    "    \n",
    "X_colloc = torch.tensor(X_colloc_np, dtype=DTYPE, device=device)\n",
    "x0_line = torch.linspace(-L, L, 500, dtype=DTYPE, device=device).unsqueeze(1)\n",
    "X0 = torch.cat([torch.zeros_like(x0_line), x0_line], dim=1)\n",
    "X_colloc = torch.cat([X_colloc, X0], dim=0)\n",
    "\n",
    "# Self-adaptive collocation mask (learned weighting of PDE residual)\n",
    "pde_logits = torch.nn.Parameter(torch.zeros((X_colloc.shape[0], 1), dtype=DTYPE, device=device))\n",
    "def current_masks(detach: bool = False):\n",
    "    pde = F.softplus(pde_logits)\n",
    "    return pde.detach() if detach else pde\n",
    "\n",
    "# Model instantiation and domain normalization\n",
    "lb = torch.tensor([0.0, -L], dtype=DTYPE, device=device)\n",
    "ub = torch.tensor([t_end,  L], dtype=DTYPE, device=device)\n",
    "model = PINN_BDNK_1D(Nl=Nl, Nn=Nn, lb=lb, ub=ub).to(device).to(DTYPE)\n",
    "model.J0_ic_func    = J0_ic_func_scaled\n",
    "model.alpha_ic_func = alpha_ic_func_scaled\n",
    "model.sJ0.copy_(torch.tensor(sJ0, dtype=DTYPE, device=device))\n",
    "model.sA.copy_(torch.tensor(sA,  dtype=DTYPE, device=device))\n",
    "\n",
    "# Weight initialization\n",
    "def glorot_normal_all_linear(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight, gain=1.0)\n",
    "        if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "model.apply(glorot_normal_all_linear)\n",
    "\n",
    "# Optimizers and learning-rate scheduler setup\n",
    "optimizer_theta = torch.optim.Adam(model.parameters(), lr=lr_net, betas=(0.9, 0.95))\n",
    "scheduler = ReduceLROnPlateau(optimizer_theta, mode='min', factor=0.4, patience=700, threshold=1e-4, min_lr=lr_net/100)\n",
    "optimizer_mask  = torch.optim.Adam([pde_logits], lr=lr_mask, betas=(0.7, 0.85), maximize=True)\n",
    "\n",
    "# Setting up and executing Adam pre-training\n",
    "def train_adam(model, optimizer_theta, optimizer_mask, epochs, print_every):\n",
    "    print(\"Starting Adam pre-training (SA-PINN with hard IC)...\")\n",
    "    best_loss, best_state = float('inf'), None\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        optimizer_theta.zero_grad()\n",
    "        optimizer_mask.zero_grad()\n",
    "\n",
    "        R = model.pde_residual(X_colloc)\n",
    "        R1, R2 = R[:, 0:1], R[:, 1:2]\n",
    "        \n",
    "        Rnorm = torch.sqrt(R1**2 + R2**2)\n",
    "        pde_mask = current_masks(detach=False)\n",
    "        L_pde = (pde_mask * Rnorm).pow(2).mean()\n",
    "\n",
    "        loss = L_pde\n",
    "        if not torch.isfinite(loss): raise RuntimeError(\"Non-finite loss detected.\")\n",
    "        \n",
    "        L_pde_phys   = (R1**2 + R2**2).mean()\n",
    "        L_total_phys = L_pde_phys\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_theta.step()\n",
    "        scheduler.step(L_total_phys.item())\n",
    "        optimizer_mask.step()\n",
    "\n",
    "        ltp = L_total_phys.detach().item()\n",
    "        loss_history.append(ltp)\n",
    "        if ltp < best_loss:\n",
    "            best_loss = ltp\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        if epoch % print_every == 0 or epoch == epochs:\n",
    "            with torch.no_grad():\n",
    "                m_pde = current_masks(detach=True)\n",
    "            print(f\"Adam Epoch {epoch}/{epochs} | \"\n",
    "                  f\"Total={loss:.3e}, PDE={L_pde.item():.3e}, <pde_mask>={m_pde.mean().item():.2f} | \"\n",
    "                  f\"Unmasked: Total={L_total_phys:.3e}, PDE={L_pde_phys.item():.3e} | \"\n",
    "                  f\"lr_net={optimizer_theta.param_groups[0]['lr']:.3e}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        with torch.enable_grad():\n",
    "            R = model.pde_residual(X_colloc)\n",
    "            L_pde_phys = ((R[:,0:1]**2 + R[:,1:2]**2)).mean()\n",
    "        print(f\"\\nAdam finished. Best loss = {best_loss:.3e} | PDE={L_pde_phys.item():.4e}\")\n",
    "    \n",
    "    return best_loss, loss_history, best_state\n",
    "    \n",
    "start_training = time.time()\n",
    "adam_loss, adam_loss_history, best_state = train_adam(model, optimizer_theta, optimizer_mask, adam_epochs, print_every=1000)\n",
    "adam_training_finished = time.time()\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "# Setting up executing L-BFGS fine-tuning\n",
    "with torch.enable_grad():\n",
    "    X_colloc.requires_grad_(True)\n",
    "    R0 = model.pde_residual(X_colloc)\n",
    "    init_lbfgs_loss = ((R0[:,0:1]**2 + R0[:,1:2]**2)).mean().detach().item()\n",
    "print(f\"LBFGS init unmasked PDE loss (from best Adam): {init_lbfgs_loss:.3e}\")\n",
    "\n",
    "loss_scale = 1.0 / max(init_lbfgs_loss, 1e-30)\n",
    "\n",
    "optimizer_lbfgs = torch.optim.LBFGS(\n",
    "    model.parameters(),\n",
    "    lr=1.0,\n",
    "    max_iter=1000,\n",
    "    max_eval=1000,\n",
    "    history_size=100,\n",
    "    line_search_fn=None,\n",
    "    tolerance_grad=1e-10,\n",
    "    tolerance_change=1e-12,\n",
    ")\n",
    "\n",
    "best = {\"loss\": float(\"inf\"), \"state\": None}\n",
    "inner_curve = []\n",
    "\n",
    "def closure():\n",
    "    optimizer_lbfgs.zero_grad(set_to_none=True)\n",
    "    X = X_colloc.requires_grad_(True)\n",
    "\n",
    "    R = model.pde_residual(X)\n",
    "    R1, R2 = R[:, 0:1], R[:, 1:2]\n",
    "    raw = (R1.pow(2) + R2.pow(2)).mean()\n",
    "\n",
    "    if not torch.isfinite(raw):\n",
    "        print(f\"NaN/Inf detected at iter {len(inner_curve)}. Exiting L-BFGS.\")\n",
    "        raise RuntimeError(\"L-BFGS_NAN\")\n",
    "\n",
    "    loss = raw * loss_scale\n",
    "\n",
    "    raw_f = float(raw)\n",
    "    if raw_f < best[\"loss\"]:\n",
    "        best[\"loss\"] = raw_f\n",
    "        best[\"state\"] = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    inner_curve.append(raw_f)\n",
    "\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "lbfgs_training_started = time.time()\n",
    "try:\n",
    "    final_loss = optimizer_lbfgs.step(closure)\n",
    "    final_raw = float(final_loss.item()) / loss_scale\n",
    "except RuntimeError as e:\n",
    "    if \"L-BFGS_NAN\" in str(e):\n",
    "        print(\"L-BFGS terminated early due to NaN/Inf.\")\n",
    "        final_raw = float(\"nan\")\n",
    "    else:\n",
    "        raise\n",
    "end_training = time.time()\n",
    "\n",
    "# Restore best L-BFGS state and record loss history\n",
    "if best[\"state\"] is not None:\n",
    "    model.load_state_dict(best[\"state\"])\n",
    "\n",
    "lbfgs_loss_history = inner_curve\n",
    "\n",
    "print(f\"LBFGS finished. final_raw={final_raw:.4e} | best_raw={best['loss']:.4e} | inner_calls={len(inner_curve)}\")\n",
    "\n",
    "print(f\"\\nTotal training time: {end_training - start_training:.4f} seconds.\"\n",
    "      f\"\\nAdam: {adam_training_finished - start_training:.4f} seconds. \"\n",
    "      f\"L-BFGS: {end_training - lbfgs_training_started:.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be52472-a9b6-49f3-b9dd-06d8c3d03159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import *\n",
    "\n",
    "plot_collocation_points(X_colloc, X_ic=None, X_bc_L=None, X_bc_R=None, L=L, t_end=t_end)\n",
    "\n",
    "J0_ic_np    = J0_ic_t.detach().cpu().numpy()\n",
    "alpha_ic_np = alpha_ic_t.detach().cpu().numpy()\n",
    "\n",
    "x_edges_eval = np.linspace(-L, L, 200+1)\n",
    "x_eval = 0.5 * (x_edges_eval[:-1] + x_edges_eval[1:])\n",
    "t_eval = np.linspace(0, t_end, 200)\n",
    "\n",
    "plot_results(\n",
    "    model,\n",
    "    t_eval=t_eval,\n",
    "    x_eval=x_eval,\n",
    "    alpha_ic=alpha_ic_t.detach().cpu().numpy(),\n",
    "    J0_ic=J0_ic_t.detach().cpu().numpy()\n",
    ")\n",
    "\n",
    "with torch.enable_grad():\n",
    "    plot_pde_residuals(model, t_eval, x_eval)\n",
    "\n",
    "lbfgs_history = {\"all_inner_per_epoch\": [lbfgs_loss_history]}\n",
    "plot_combined_loss_history(adam_loss_history, lbfgs_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f2a95-5eb7-476b-806a-22820b9f6ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8584653f-fe9f-480f-88b0-be1c3c398024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
