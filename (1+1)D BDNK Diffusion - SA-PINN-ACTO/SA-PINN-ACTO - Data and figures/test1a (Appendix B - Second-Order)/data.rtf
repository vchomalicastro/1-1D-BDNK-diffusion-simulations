{\rtf1\ansi\ansicpg1252\cocoartf2821
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww18140\viewh12120\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 import time\
\
# Device configuration and core PyTorch setup\
import torch\
import torch.nn as nn\
import torch.nn.functional as F\
DTYPE = torch.float32\
device = torch.device('cuda')\
if device.type == "cuda":\
    torch.cuda.init()\
    torch.rand(1, device=device)\
print(f"Using device: \{device\}")\
\
from torch.optim.lr_scheduler import ReduceLROnPlateau\
import numpy as np\
import matplotlib.pyplot as plt\
from SA_PINN_ACTO import PINN_BDNK_1D\
from IC_1D import IC_BDNK\
\
# Hyperparameters for network architecture and training schedule\
Nl, Nn = 10, 70\
t_end = 20.0\
L = 50.0\
adam_epochs = 50_000 #30_000 #30\
lr_net = 1e-3 #3e-3\
lr_mask = 8e-2\
\
# Sampling parameters and domain sampling\
N_colloc = 20_000\
N_ic = 1000\
N_bc = 1000\
\
def lhs_box(n, low, high, rng=np.random):\
    low, high = np.asarray(low, float), np.asarray(high, float)\
    D = low.size\
    H = np.empty((n, D), float)\
    for j in range(D):\
        P = (rng.permutation(n) + rng.rand(n)) / n\
        H[:, j] = low[j] + P * (high[j] - low[j])\
    return H\
\
X_colloc_np = lhs_box(N_colloc, low=np.array([0.0, -L]), high=np.array([t_end, L])).astype(np.float32)\
\
# Construction of initial condition sampling grid\
x_edges = np.linspace(-L, L, N_ic+1)\
x_ic = (0.5 * (x_edges[:-1] + x_edges[1:])).reshape(-1, 1)\
t_ic = np.zeros_like(x_ic)\
X_ic = np.hstack((t_ic, x_ic))\
\
X_ic_t = torch.tensor(X_ic, dtype=DTYPE, device=device)\
alpha1st_ic_t, alpha2nd_ic_t = IC_BDNK(X_ic_t, L)\
\
# Scaling factors for numerical stability of IC enforcement\
with torch.no_grad():\
    sA = alpha1st_ic_t.abs().max().clamp_min(1e-12).item()\
print(f"[scales] sA=\{sA:.3e\}")\
\
# Sorting IC data to enable fast 1D interpolation\
x_ic_torch = X_ic_t[:, 1:2].contiguous().view(-1)\
x_sorted, idx_sort = torch.sort(x_ic_torch)\
alpha1st_sorted = alpha1st_ic_t.view(-1)[idx_sort]\
alpha2nd_sorted = alpha2nd_ic_t.view(-1)[idx_sort]\
\
@torch.no_grad()\
def _torch_lin_interp_1d(xq: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\
    xq_flat = xq.view(-1)\
    xq_clamped = xq_flat.clamp(min=x[0], max=x[-1])\
    idx_hi = torch.searchsorted(x, xq_clamped, right=True)\
    idx_hi = idx_hi.clamp(min=1, max=x.numel() - 1)\
    idx_lo = idx_hi - 1\
    x0 = x[idx_lo]; x1 = x[idx_hi]\
    y0 = y[idx_lo]; y1 = y[idx_hi]\
    denom = (x1 - x0)\
    denom = torch.where(denom.abs() > 0, denom, torch.ones_like(denom))\
    w = (xq_clamped - x0) / denom\
    yq = y0 + w * (y1 - y0)\
    return yq.view_as(xq)\
\
# Initial condition functions passed to the neural network (physical scale)\
def alpha1st_ic_func(x_phys: torch.Tensor) -> torch.Tensor:\
    if x_sorted.device != x_phys.device or x_sorted.dtype != x_phys.dtype:\
        xk = x_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\
        yk = alpha1st_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\
    else:\
        xk, yk = x_sorted, alpha1st_sorted\
    yq = _torch_lin_interp_1d(x_phys.view(-1), xk, yk)\
    return yq.view(-1, 1)\
\
def alpha2nd_ic_func(x_phys: torch.Tensor) -> torch.Tensor:\
    if x_sorted.device != x_phys.device or x_sorted.dtype != x_phys.dtype:\
        xk = x_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\
        yk = alpha2nd_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\
    else:\
        xk, yk = x_sorted, alpha2nd_sorted\
    yq = _torch_lin_interp_1d(x_phys.view(-1), xk, yk)\
    return yq.view(-1, 1)\
\
# Scaled initial condition functions (used internally by the model)\
def alpha1st_ic_func_scaled(x_phys: torch.Tensor) -> torch.Tensor:\
    return alpha1st_ic_func(x_phys) / sA\
\
def alpha2nd_ic_func_scaled(x_phys: torch.Tensor) -> torch.Tensor:\
    return alpha2nd_ic_func(x_phys) / sA\
\
xL = -L * np.ones((N_bc, 1))\
xR =  L * np.ones((N_bc, 1))\
t_bc = np.random.uniform(0.0, t_end, size=(N_bc, 1))\
X_bc_L = np.hstack((t_bc, xL))\
X_bc_R = np.hstack((t_bc, xR))\
\
X_colloc = torch.tensor(X_colloc_np, dtype=DTYPE, device=device)\
x0_line = torch.linspace(-L, L, 500, dtype=DTYPE, device=device).unsqueeze(1)\
X0 = torch.cat([torch.zeros_like(x0_line), x0_line], dim=1)\
X_colloc = torch.cat([X_colloc, X0], dim=0)\
\
X_bc_L = torch.tensor(X_bc_L, dtype=DTYPE, device=device)\
X_bc_R = torch.tensor(X_bc_R, dtype=DTYPE, device=device)\
\
Nt = 100\
x_mass = torch.tensor(x_ic.flatten(), dtype=DTYPE, device=device)\
t_mass = torch.linspace(0, t_end, Nt, dtype=DTYPE, device=device)\
\
# Self-adaptive collocation mask (learned weighting of PDE residual)\
pde_logits = torch.nn.Parameter(torch.zeros((X_colloc.shape[0], 1), dtype=DTYPE, device=device))\
def current_masks(detach: bool = False):\
    pde = F.softplus(pde_logits)\
    return pde.detach() if detach else pde\
\
# Model instantiation and domain normalization\
lb = torch.tensor([0.0, -L], dtype=DTYPE, device=device)\
ub = torch.tensor([t_end,  L], dtype=DTYPE, device=device)\
model = PINN_BDNK_1D(Nl=Nl, Nn=Nn, lb=lb, ub=ub).to(device).to(DTYPE)\
model.alpha_ic_func = alpha1st_ic_func_scaled\
model.sA.copy_(torch.tensor(sA, dtype=DTYPE, device=device))\
\
# Weight initialization\
def glorot_normal_all_linear(m):\
    if isinstance(m, nn.Linear):\
        nn.init.xavier_normal_(m.weight, gain=1.0)\
        if m.bias is not None: nn.init.zeros_(m.bias)\
model.apply(glorot_normal_all_linear)\
\
# Optimizers and learning-rate scheduler setup\
optimizer_theta = torch.optim.Adam(model.parameters(), lr=lr_net, betas=(0.9, 0.95))\
scheduler = ReduceLROnPlateau(optimizer_theta, mode='min', factor=0.4, patience=2000, threshold=1e-4, min_lr=lr_net/100)\
optimizer_mask  = torch.optim.Adam([pde_logits], lr=lr_mask, betas=(0.8, 0.9), maximize=True)\
\
# Setting up and executing Adam pre-training\
def train_adam(model, optimizer_theta, optimizer_mask, epochs, print_every):\
    print("Starting Adam pre-training (SA-PINN with hard IC)...")\
    best_loss, best_state = float('inf'), None\
    loss_history = []\
\
    for epoch in range(1, epochs + 1):\
        optimizer_theta.zero_grad()\
        optimizer_mask.zero_grad()\
\
        R = model.pde_residual(X_colloc)\
        Rnorm = R.abs()\
        pde_mask = current_masks(detach=False)\
        L_pde = (pde_mask * Rnorm).pow(2).mean()\
\
        L_ic = model.loss_ic(X_ic_t, alpha2nd_ic_t)\
        loss = L_pde + L_ic\
        if not torch.isfinite(loss): raise RuntimeError("Non-finite loss detected.")\
\
        L_pde_phys   = (R**2).mean()\
        L_total_phys = L_pde_phys + L_ic\
\
        loss.backward()\
        optimizer_theta.step()\
        scheduler.step(L_total_phys.item())\
        optimizer_mask.step()\
\
        ltp = L_total_phys.detach().item()\
        loss_history.append(ltp)\
        if ltp < best_loss:\
            best_loss = ltp\
            best_state = \{k: v.detach().cpu().clone() for k, v in model.state_dict().items()\}\
\
        if epoch % print_every == 0 or epoch == epochs:\
            with torch.no_grad():\
                m_pde = current_masks(detach=True)\
            print(f"Adam Epoch \{epoch\}/\{epochs\} | "\
                  f"Total=\{loss:.3e\}, PDE=\{L_pde.item():.3e\}, <pde_mask>=\{m_pde.mean().item():.2f\} | "\
                  f"Unmasked: Total=\{L_total_phys:.3e\}, PDE=\{L_pde_phys.item():.3e\}, IC=\{L_ic:.3e\} | "\
                  f"lr_net=\{optimizer_theta.param_groups[0]['lr']:.3e\}")\
\
    if best_state is not None:\
        model.load_state_dict(best_state)\
        with torch.enable_grad():\
            R = model.pde_residual(X_colloc)\
            L_pde_phys = (R**2).mean()\
            L_ic = model.loss_ic(X_ic_t, alpha2nd_ic_t)\
        print(f"\\nAdam finished. Best loss = \{best_loss:.3e\} | PDE=\{L_pde_phys.item():.4e\}, IC=\{L_ic.item():.4e\}")\
\
    return best_loss, loss_history, best_state\
\
start_training = time.time()\
adam_loss, adam_loss_history, best_state = train_adam(model, optimizer_theta, optimizer_mask, adam_epochs, print_every=1000)\
adam_training_finished = time.time()\
\
model.load_state_dict(best_state)\
\
# Setting up executing L-BFGS fine-tuning\
with torch.enable_grad():\
    X_colloc_tmp = X_colloc.clone().detach().requires_grad_(True)\
    R0   = model.pde_residual(X_colloc_tmp)\
    L_IC = model.loss_ic(X_ic_t, alpha2nd_ic_t)\
    init_lbfgs_loss = (R0**2).mean().detach().item() + L_IC.detach().item()\
\
print(f"LBFGS init unmasked PDE loss (from best Adam): \{init_lbfgs_loss:.3e\}")\
\
loss_scale = 1.0 / max(init_lbfgs_loss, 1e-30)\
\
optimizer_lbfgs = torch.optim.LBFGS(\
    model.parameters(),\
    lr=1.0,\
    max_iter=2000,\
    max_eval=2000,\
    history_size=1000,\
    line_search_fn=None,\
    tolerance_grad=1e-10,\
    tolerance_change=1e-12,\
)\
\
best = \{"loss": float("inf"), "state": None\}\
inner_curve = []\
\
def closure():\
    optimizer_lbfgs.zero_grad(set_to_none=True)\
    X = X_colloc.requires_grad_(True)\
\
    R = model.pde_residual(X)\
    L_ic = model.loss_ic(X_ic_t, alpha2nd_ic_t)\
    raw = (R.pow(2)).mean() + L_ic\
\
    if not torch.isfinite(raw):\
        print(f"NaN/Inf detected at iter \{len(inner_curve)\}. Exiting L-BFGS.")\
        raise RuntimeError("L-BFGS_NAN")\
\
    loss = raw * loss_scale\
\
    raw_f = float(raw)\
    if raw_f < best["loss"]:\
        best["loss"] = raw_f\
        best["state"] = \{k: v.detach().cpu().clone() for k, v in model.state_dict().items()\}\
\
    inner_curve.append(raw_f)\
\
    loss.backward()\
    return loss\
\
lbfgs_training_started = time.time()\
try:\
    final_loss = optimizer_lbfgs.step(closure)\
    final_raw = float(final_loss.item()) / loss_scale\
except RuntimeError as e:\
    if "L-BFGS_NAN" in str(e):\
        print("L-BFGS terminated early due to NaN/Inf.")\
        final_raw = float("nan")\
    else:\
        raise\
end_training = time.time()\
\
# Restore best L-BFGS state and record loss history\
if best["state"] is not None:\
    model.load_state_dict(best["state"])\
\
with torch.enable_grad():\
    X_eval = X_colloc.clone().detach().requires_grad_(True)\
    R_eval = model.pde_residual(X_eval)\
    L_pde_best = (R_eval**2).mean().detach().item()\
    L_ic_best = model.loss_ic(X_ic_t, alpha2nd_ic_t).detach().item()\
    total_best = L_pde_best + L_ic_best\
\
lbfgs_loss_history = inner_curve\
\
print(f"LBFGS finished. Final total loss=\{final_raw:.4e\} | Best total loss=\{best['loss']:.4e\} | "\
      f"PDE=\{L_pde_best:.4e\} | IC=\{L_ic_best:.4e\} | Iterations=\{len(inner_curve)\}")\
\
print(f"\\nTotal training time: \{end_training - start_training:.4f\} seconds."\
      f"\\nAdam: \{adam_training_finished - start_training:.4f\} seconds. "\
      f"L-BFGS: \{end_training - lbfgs_training_started:.4f\} seconds.")\
\
[\'85]\
Output:\
\
Using device: cuda\
[scales] sA=2.150e+01\
Starting Adam pre-training (SA-PINN with hard IC)...\
Adam Epoch 1000/50000 | Total=9.142e-03, PDE=8.536e-03, <pde_mask>=17.24 | Unmasked: Total=6.101e-04, PDE=4.023e-06, IC=6.061e-04 | lr_net=1.000e-03\
Adam Epoch 2000/50000 | Total=6.981e-03, PDE=6.710e-03, <pde_mask>=24.39 | Unmasked: Total=2.715e-04, PDE=9.047e-07, IC=2.706e-04 | lr_net=1.000e-03\
Adam Epoch 3000/50000 | Total=2.029e-02, PDE=1.998e-02, <pde_mask>=30.48 | Unmasked: Total=3.102e-04, PDE=1.174e-06, IC=3.091e-04 | lr_net=1.000e-03\
Adam Epoch 4000/50000 | Total=1.432e-03, PDE=1.406e-03, <pde_mask>=35.94 | Unmasked: Total=2.647e-05, PDE=1.141e-07, IC=2.636e-05 | lr_net=1.000e-03\
Adam Epoch 5000/50000 | Total=1.938e-03, PDE=1.914e-03, <pde_mask>=41.31 | Unmasked: Total=2.468e-05, PDE=9.192e-08, IC=2.459e-05 | lr_net=1.000e-03\
Adam Epoch 6000/50000 | Total=2.467e-02, PDE=2.459e-02, <pde_mask>=46.51 | Unmasked: Total=8.388e-05, PDE=4.283e-07, IC=8.345e-05 | lr_net=1.000e-03\
Adam Epoch 7000/50000 | Total=3.509e-03, PDE=3.469e-03, <pde_mask>=50.99 | Unmasked: Total=3.981e-05, PDE=6.415e-08, IC=3.974e-05 | lr_net=4.000e-04\
Adam Epoch 8000/50000 | Total=3.972e-03, PDE=3.945e-03, <pde_mask>=53.97 | Unmasked: Total=2.697e-05, PDE=5.067e-08, IC=2.692e-05 | lr_net=4.000e-04\
Adam Epoch 9000/50000 | Total=3.963e-04, PDE=3.591e-04, <pde_mask>=56.38 | Unmasked: Total=3.722e-05, PDE=9.636e-09, IC=3.721e-05 | lr_net=1.600e-04\
Adam Epoch 10000/50000 | Total=5.599e-04, PDE=5.328e-04, <pde_mask>=57.59 | Unmasked: Total=2.705e-05, PDE=9.960e-09, IC=2.704e-05 | lr_net=1.600e-04\
Adam Epoch 11000/50000 | Total=4.263e-04, PDE=3.994e-04, <pde_mask>=58.62 | Unmasked: Total=2.684e-05, PDE=8.456e-09, IC=2.683e-05 | lr_net=6.400e-05\
Adam Epoch 12000/50000 | Total=2.963e-04, PDE=2.744e-04, <pde_mask>=59.12 | Unmasked: Total=2.193e-05, PDE=8.271e-09, IC=2.192e-05 | lr_net=6.400e-05\
Adam Epoch 13000/50000 | Total=8.470e-05, PDE=6.968e-05, <pde_mask>=59.57 | Unmasked: Total=1.502e-05, PDE=7.563e-09, IC=1.501e-05 | lr_net=2.560e-05\
Adam Epoch 14000/50000 | Total=8.822e-05, PDE=7.600e-05, <pde_mask>=59.88 | Unmasked: Total=1.223e-05, PDE=8.285e-09, IC=1.222e-05 | lr_net=2.560e-05\
Adam Epoch 15000/50000 | Total=7.213e-05, PDE=6.233e-05, <pde_mask>=60.18 | Unmasked: Total=9.818e-06, PDE=8.540e-09, IC=9.809e-06 | lr_net=1.024e-05\
Adam Epoch 16000/50000 | Total=6.746e-05, PDE=5.934e-05, <pde_mask>=60.46 | Unmasked: Total=8.132e-06, PDE=8.732e-09, IC=8.123e-06 | lr_net=1.024e-05\
Adam Epoch 17000/50000 | Total=6.497e-05, PDE=5.781e-05, <pde_mask>=60.72 | Unmasked: Total=7.172e-06, PDE=8.632e-09, IC=7.163e-06 | lr_net=1.000e-05\
Adam Epoch 18000/50000 | Total=5.593e-05, PDE=4.907e-05, <pde_mask>=60.98 | Unmasked: Total=6.863e-06, PDE=8.379e-09, IC=6.855e-06 | lr_net=1.000e-05\
Adam Epoch 19000/50000 | Total=5.723e-05, PDE=5.078e-05, <pde_mask>=61.23 | Unmasked: Total=6.453e-06, PDE=8.103e-09, IC=6.445e-06 | lr_net=1.000e-05\
Adam Epoch 20000/50000 | Total=5.651e-05, PDE=5.011e-05, <pde_mask>=61.47 | Unmasked: Total=6.408e-06, PDE=7.855e-09, IC=6.400e-06 | lr_net=1.000e-05\
Adam Epoch 21000/50000 | Total=4.946e-05, PDE=4.342e-05, <pde_mask>=61.70 | Unmasked: Total=6.041e-06, PDE=7.477e-09, IC=6.033e-06 | lr_net=1.000e-05\
Adam Epoch 22000/50000 | Total=4.831e-05, PDE=4.259e-05, <pde_mask>=61.92 | Unmasked: Total=5.729e-06, PDE=7.166e-09, IC=5.722e-06 | lr_net=1.000e-05\
Adam Epoch 23000/50000 | Total=4.393e-05, PDE=3.838e-05, <pde_mask>=62.13 | Unmasked: Total=5.565e-06, PDE=6.862e-09, IC=5.558e-06 | lr_net=1.000e-05\
Adam Epoch 24000/50000 | Total=4.073e-05, PDE=3.517e-05, <pde_mask>=62.34 | Unmasked: Total=5.573e-06, PDE=6.589e-09, IC=5.566e-06 | lr_net=1.000e-05\
Adam Epoch 25000/50000 | Total=4.951e-05, PDE=4.395e-05, <pde_mask>=62.54 | Unmasked: Total=5.566e-06, PDE=6.400e-09, IC=5.560e-06 | lr_net=1.000e-05\
Adam Epoch 26000/50000 | Total=4.125e-05, PDE=3.593e-05, <pde_mask>=62.73 | Unmasked: Total=5.326e-06, PDE=6.061e-09, IC=5.320e-06 | lr_net=1.000e-05\
Adam Epoch 27000/50000 | Total=3.593e-05, PDE=3.096e-05, <pde_mask>=62.91 | Unmasked: Total=4.982e-06, PDE=5.735e-09, IC=4.976e-06 | lr_net=1.000e-05\
Adam Epoch 28000/50000 | Total=4.203e-05, PDE=3.728e-05, <pde_mask>=63.09 | Unmasked: Total=4.763e-06, PDE=5.505e-09, IC=4.758e-06 | lr_net=1.000e-05\
Adam Epoch 29000/50000 | Total=4.444e-05, PDE=3.943e-05, <pde_mask>=63.26 | Unmasked: Total=5.020e-06, PDE=5.353e-09, IC=5.015e-06 | lr_net=1.000e-05\
Adam Epoch 30000/50000 | Total=3.790e-05, PDE=3.308e-05, <pde_mask>=63.42 | Unmasked: Total=4.830e-06, PDE=5.078e-09, IC=4.825e-06 | lr_net=1.000e-05\
Adam Epoch 31000/50000 | Total=3.846e-05, PDE=3.404e-05, <pde_mask>=63.58 | Unmasked: Total=4.426e-06, PDE=4.818e-09, IC=4.421e-06 | lr_net=1.000e-05\
Adam Epoch 32000/50000 | Total=3.126e-05, PDE=2.676e-05, <pde_mask>=63.74 | Unmasked: Total=4.498e-06, PDE=4.628e-09, IC=4.493e-06 | lr_net=1.000e-05\
Adam Epoch 33000/50000 | Total=3.954e-05, PDE=3.505e-05, <pde_mask>=63.89 | Unmasked: Total=4.498e-06, PDE=4.512e-09, IC=4.493e-06 | lr_net=1.000e-05\
Adam Epoch 34000/50000 | Total=2.730e-05, PDE=2.308e-05, <pde_mask>=64.03 | Unmasked: Total=4.231e-06, PDE=4.252e-09, IC=4.227e-06 | lr_net=1.000e-05\
Adam Epoch 35000/50000 | Total=3.384e-05, PDE=2.953e-05, <pde_mask>=64.17 | Unmasked: Total=4.313e-06, PDE=4.185e-09, IC=4.308e-06 | lr_net=1.000e-05\
Adam Epoch 36000/50000 | Total=2.671e-05, PDE=2.258e-05, <pde_mask>=64.30 | Unmasked: Total=4.133e-06, PDE=3.991e-09, IC=4.129e-06 | lr_net=1.000e-05\
Adam Epoch 37000/50000 | Total=3.109e-05, PDE=2.726e-05, <pde_mask>=64.43 | Unmasked: Total=3.839e-06, PDE=3.853e-09, IC=3.835e-06 | lr_net=1.000e-05\
Adam Epoch 38000/50000 | Total=3.738e-05, PDE=3.368e-05, <pde_mask>=64.55 | Unmasked: Total=3.707e-06, PDE=3.776e-09, IC=3.703e-06 | lr_net=1.000e-05\
Adam Epoch 39000/50000 | Total=2.571e-05, PDE=2.185e-05, <pde_mask>=64.67 | Unmasked: Total=3.863e-06, PDE=3.666e-09, IC=3.859e-06 | lr_net=1.000e-05\
Adam Epoch 40000/50000 | Total=3.026e-05, PDE=2.642e-05, <pde_mask>=64.79 | Unmasked: Total=3.845e-06, PDE=3.631e-09, IC=3.841e-06 | lr_net=1.000e-05\
Adam Epoch 41000/50000 | Total=2.236e-05, PDE=1.878e-05, <pde_mask>=64.90 | Unmasked: Total=3.576e-06, PDE=3.473e-09, IC=3.573e-06 | lr_net=1.000e-05\
Adam Epoch 42000/50000 | Total=3.083e-05, PDE=2.739e-05, <pde_mask>=65.01 | Unmasked: Total=3.438e-06, PDE=3.445e-09, IC=3.435e-06 | lr_net=1.000e-05\
Adam Epoch 43000/50000 | Total=2.837e-05, PDE=2.502e-05, <pde_mask>=65.12 | Unmasked: Total=3.358e-06, PDE=3.384e-09, IC=3.354e-06 | lr_net=1.000e-05\
Adam Epoch 44000/50000 | Total=2.166e-05, PDE=1.815e-05, <pde_mask>=65.22 | Unmasked: Total=3.510e-06, PDE=3.353e-09, IC=3.507e-06 | lr_net=1.000e-05\
Adam Epoch 45000/50000 | Total=1.893e-05, PDE=1.560e-05, <pde_mask>=65.33 | Unmasked: Total=3.334e-06, PDE=3.285e-09, IC=3.331e-06 | lr_net=1.000e-05\
Adam Epoch 46000/50000 | Total=1.936e-05, PDE=1.608e-05, <pde_mask>=65.42 | Unmasked: Total=3.277e-06, PDE=3.257e-09, IC=3.274e-06 | lr_net=1.000e-05\
Adam Epoch 47000/50000 | Total=2.037e-05, PDE=1.707e-05, <pde_mask>=65.52 | Unmasked: Total=3.311e-06, PDE=3.273e-09, IC=3.308e-06 | lr_net=1.000e-05\
Adam Epoch 48000/50000 | Total=2.141e-05, PDE=1.830e-05, <pde_mask>=65.61 | Unmasked: Total=3.118e-06, PDE=3.229e-09, IC=3.115e-06 | lr_net=1.000e-05\
Adam Epoch 49000/50000 | Total=2.048e-05, PDE=1.727e-05, <pde_mask>=65.70 | Unmasked: Total=3.212e-06, PDE=3.249e-09, IC=3.209e-06 | lr_net=1.000e-05\
Adam Epoch 50000/50000 | Total=2.023e-05, PDE=1.712e-05, <pde_mask>=65.79 | Unmasked: Total=3.114e-06, PDE=3.228e-09, IC=3.111e-06 | lr_net=1.000e-05\
\
Adam finished. Best loss = 2.962e-06 | PDE=3.2681e-09, IC=3.1778e-06\
LBFGS init unmasked PDE loss (from best Adam): 3.181e-06\
NaN/Inf detected at iter 1701. Exiting L-BFGS.\
L-BFGS terminated early due to NaN/Inf.\
LBFGS finished. Final total loss=nan | Best total loss=3.0559e-09 | PDE=2.3450e-09 | IC=7.1091e-10 | Iterations=1701\
\
Total training time: 1328.4257 seconds.\
Adam: 1216.0285 seconds. L-BFGS: 112.3862 seconds.}