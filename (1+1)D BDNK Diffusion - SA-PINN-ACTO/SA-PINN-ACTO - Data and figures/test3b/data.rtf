{\rtf1\ansi\ansicpg1252\cocoartf2821
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww18140\viewh12120\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 import time\
\
# Device configuration and core PyTorch setup\
import torch\
import torch.nn as nn\
import torch.nn.functional as F\
DTYPE = torch.float32\
device = torch.device('cuda')\
if device.type == "cuda":\
    torch.cuda.init()\
    torch.rand(1, device=device)\
print(f"Using device: \{device\}")\
\
from torch.optim.lr_scheduler import ReduceLROnPlateau\
import numpy as np\
import matplotlib.pyplot as plt\
from SA_PINN_ACTO import PINN_BDNK_1D\
from IC_1D import IC_BDNK\
\
# Hyperparameters for network architecture and training schedule\
Nl, Nn = 10, 70\
t_end = 20.0\
L = 50.0\
adam_epochs = 25_000\
lr_net = 5e-3\
lr_mask = 4e-2 \
\
# BDNK simulation configuration and background field setup\
from BDNK_Functions import *\
BDNK_simulation = 2\
setup_external_Tv(BDNK_simulation, L)\
\
# Sampling parameters and domain sampling\
N_colloc = 20000\
# In the ACTO case, the N_ic below are not collocation points where an IC residual will be computed,\
# but points at which the exact initial condition gets computed\
N_ic = 1000\
\
def lhs_box(n, low, high, rng=np.random):\
    low, high = np.asarray(low, float), np.asarray(high, float)\
    D = low.size\
    H = np.empty((n, D), float)\
    for j in range(D):\
        P = (rng.permutation(n) + rng.rand(n)) / n\
        H[:, j] = low[j] + P * (high[j] - low[j])\
    return H\
    \
X_colloc_np = lhs_box(N_colloc, low=np.array([0.0, -L]), high=np.array([t_end, L])).astype(np.float32)\
\
# Construction of initial condition sampling grid\
x_edges = np.linspace(-L, L, N_ic+1)\
x_ic = (0.5 * (x_edges[:-1] + x_edges[1:])).reshape(-1, 1)\
t_ic = np.zeros_like(x_ic)\
X_ic = np.hstack((t_ic, x_ic))\
\
X_ic_t = torch.tensor(X_ic, dtype=DTYPE, device=device)\
J0_ic_t, alpha_ic_t, _ = IC_BDNK(X_ic_t, L)\
\
# Scaling factors for numerical stability of IC enforcement\
with torch.no_grad():\
    sJ0 = J0_ic_t.abs().max().clamp_min(1e-12).item()\
    sA  = alpha_ic_t.abs().max().clamp_min(1e-12).item()\
print(f"[scales] sJ0=\{sJ0:.3e\}, sA=\{sA:.3e\}")\
\
# Sorting IC data to enable fast 1D interpolation\
x_ic_torch = X_ic_t[:, 1:2].contiguous().view(-1)\
x_sorted, idx_sort = torch.sort(x_ic_torch)\
J0_sorted    = J0_ic_t.view(-1)[idx_sort]\
alpha_sorted = alpha_ic_t.view(-1)[idx_sort]\
\
@torch.no_grad()\
def _torch_lin_interp_1d(xq: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\
    xq_flat = xq.view(-1)\
    xq_clamped = xq_flat.clamp(min=x[0], max=x[-1])\
    idx_hi = torch.searchsorted(x, xq_clamped, right=True)\
    idx_hi = idx_hi.clamp(min=1, max=x.numel() - 1)\
    idx_lo = idx_hi - 1\
    x0 = x[idx_lo]; x1 = x[idx_hi]\
    y0 = y[idx_lo]; y1 = y[idx_hi]\
    denom = (x1 - x0)\
    denom = torch.where(denom.abs() > 0, denom, torch.ones_like(denom))\
    w = (xq_clamped - x0) / denom\
    yq = y0 + w * (y1 - y0)\
    return yq.view_as(xq)\
\
# Initial condition functions passed to the neural network (physical scale)\
def J0_ic_func(x_phys: torch.Tensor) -> torch.Tensor:\
    if x_sorted.device != x_phys.device or x_sorted.dtype != x_phys.dtype:\
        xk = x_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\
        yk = J0_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\
    else:\
        xk, yk = x_sorted, J0_sorted\
    yq = _torch_lin_interp_1d(x_phys.view(-1), xk, yk)\
    return yq.view(-1, 1)\
\
def alpha_ic_func(x_phys: torch.Tensor) -> torch.Tensor:\
    if x_sorted.device != x_phys.device or x_sorted.dtype != x_phys.dtype:\
        xk = x_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\
        yk = alpha_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\
    else:\
        xk, yk = x_sorted, alpha_sorted\
    yq = _torch_lin_interp_1d(x_phys.view(-1), xk, yk)\
    return yq.view(-1, 1)\
\
# Scaled initial condition functions (used internally by the model)\
def J0_ic_func_scaled(x_phys: torch.Tensor) -> torch.Tensor:\
    return J0_ic_func(x_phys) / sJ0\
\
def alpha_ic_func_scaled(x_phys: torch.Tensor) -> torch.Tensor:\
    return alpha_ic_func(x_phys) / sA\
    \
X_colloc = torch.tensor(X_colloc_np, dtype=DTYPE, device=device)\
x0_line = torch.linspace(-L, L, 500, dtype=DTYPE, device=device).unsqueeze(1)\
X0 = torch.cat([torch.zeros_like(x0_line), x0_line], dim=1)\
X_colloc = torch.cat([X_colloc, X0], dim=0)\
\
# Self-adaptive collocation mask (learned weighting of PDE residual)\
pde_logits = torch.nn.Parameter(torch.zeros((X_colloc.shape[0], 1), dtype=DTYPE, device=device))\
def current_masks(detach: bool = False):\
    pde = F.softplus(pde_logits)\
    return pde.detach() if detach else pde\
\
# Model instantiation and domain normalization\
lb = torch.tensor([0.0, -L], dtype=DTYPE, device=device)\
ub = torch.tensor([t_end,  L], dtype=DTYPE, device=device)\
model = PINN_BDNK_1D(Nl=Nl, Nn=Nn, lb=lb, ub=ub).to(device).to(DTYPE)\
model.J0_ic_func    = J0_ic_func_scaled\
model.alpha_ic_func = alpha_ic_func_scaled\
model.sJ0.copy_(torch.tensor(sJ0, dtype=DTYPE, device=device))\
model.sA.copy_(torch.tensor(sA,  dtype=DTYPE, device=device))\
\
# Weight initialization\
def glorot_normal_all_linear(m):\
    if isinstance(m, nn.Linear):\
        nn.init.xavier_normal_(m.weight, gain=1.0)\
        if m.bias is not None: nn.init.zeros_(m.bias)\
model.apply(glorot_normal_all_linear)\
\
# Optimizers and learning-rate scheduler setup\
optimizer_theta = torch.optim.Adam(model.parameters(), lr=lr_net, betas=(0.9, 0.95))\
scheduler = ReduceLROnPlateau(optimizer_theta, mode='min', factor=0.4, patience=700, threshold=1e-4, min_lr=lr_net/100)\
optimizer_mask  = torch.optim.Adam([pde_logits], lr=lr_mask, betas=(0.7, 0.85), maximize=True)\
\
# Setting up and executing Adam pre-training\
def train_adam(model, optimizer_theta, optimizer_mask, epochs, print_every):\
    print("Starting Adam pre-training (SA-PINN with hard IC)...")\
    best_loss, best_state = float('inf'), None\
    loss_history = []\
\
    for epoch in range(1, epochs + 1):\
        optimizer_theta.zero_grad()\
        optimizer_mask.zero_grad()\
\
        R = model.pde_residual(X_colloc)\
        R1, R2 = R[:, 0:1], R[:, 1:2]\
        \
        Rnorm = torch.sqrt(R1**2 + R2**2)\
        pde_mask = current_masks(detach=False)\
        L_pde = (pde_mask * Rnorm).pow(2).mean()\
\
        loss = L_pde\
        if not torch.isfinite(loss): raise RuntimeError("Non-finite loss detected.")\
        \
        L_pde_phys   = (R1**2 + R2**2).mean()\
        L_total_phys = L_pde_phys\
\
        loss.backward()\
        optimizer_theta.step()\
        scheduler.step(L_total_phys.item())\
        optimizer_mask.step()\
\
        ltp = L_total_phys.detach().item()\
        loss_history.append(ltp)\
        if ltp < best_loss:\
            best_loss = ltp\
            best_state = \{k: v.detach().cpu().clone() for k, v in model.state_dict().items()\}\
\
        if epoch % print_every == 0 or epoch == epochs:\
            with torch.no_grad():\
                m_pde = current_masks(detach=True)\
            print(f"Adam Epoch \{epoch\}/\{epochs\} | "\
                  f"Total=\{loss:.3e\}, PDE=\{L_pde.item():.3e\}, <pde_mask>=\{m_pde.mean().item():.2f\} | "\
                  f"Unmasked: Total=\{L_total_phys:.3e\}, PDE=\{L_pde_phys.item():.3e\} | "\
                  f"lr_net=\{optimizer_theta.param_groups[0]['lr']:.3e\}")\
\
    if best_state is not None:\
        model.load_state_dict(best_state)\
        with torch.enable_grad():\
            R = model.pde_residual(X_colloc)\
            L_pde_phys = ((R[:,0:1]**2 + R[:,1:2]**2)).mean()\
        print(f"\\nAdam finished. Best loss = \{best_loss:.3e\} | PDE=\{L_pde_phys.item():.4e\}")\
    \
    return best_loss, loss_history, best_state\
    \
start_training = time.time()\
adam_loss, adam_loss_history, best_state = train_adam(model, optimizer_theta, optimizer_mask, adam_epochs, print_every=1000)\
adam_training_finished = time.time()\
\
model.load_state_dict(best_state)\
\
# Setting up executing L-BFGS fine-tuning\
with torch.enable_grad():\
    X_colloc.requires_grad_(True)\
    res = model.pde_residual(X_colloc)\
    init_lbfgs_loss = ((res[:,0:1]**2 + res[:,1:2]**2)).mean().detach().item()\
print(f"LBFGS init unmasked PDE loss (from best Adam): \{init_lbfgs_loss:.3e\}")\
\
loss_scale = 1.0 / max(init_lbfgs_loss, 1e-30)\
\
optimizer_lbfgs = torch.optim.LBFGS(\
    model.parameters(),\
    lr=1.0,\
    max_iter=1000,\
    max_eval=1000,\
    history_size=100,\
    line_search_fn=None,\
    tolerance_grad=1e-10,\
    tolerance_change=1e-12,\
)\
\
best = \{"loss": float("inf"), "state": None\}\
inner_curve = []\
\
def closure():\
    optimizer_lbfgs.zero_grad(set_to_none=True)\
    X = X_colloc.requires_grad_(True)\
\
    R = model.pde_residual(X)\
    R1, R2 = R[:, 0:1], R[:, 1:2]\
    raw = (R1.pow(2) + R2.pow(2)).mean()\
\
    if not torch.isfinite(raw):\
        print(f"NaN/Inf detected at iter \{len(inner_curve)\}. Exiting L-BFGS.")\
        raise RuntimeError("L-BFGS_NAN")\
\
    loss = raw * loss_scale\
\
    raw_f = float(raw)\
    if raw_f < best["loss"]:\
        best["loss"] = raw_f\
        best["state"] = \{k: v.detach().cpu().clone() for k, v in model.state_dict().items()\}\
\
    inner_curve.append(raw_f)\
\
    loss.backward()\
    return loss\
\
lbfgs_training_started = time.time()\
try:\
    final_loss = optimizer_lbfgs.step(closure)\
    final_raw = float(final_loss.item()) / loss_scale\
except RuntimeError as e:\
    if "L-BFGS_NAN" in str(e):\
        print("L-BFGS terminated early due to NaN/Inf.")\
        final_raw = float("nan")\
    else:\
        raise\
end_training = time.time()\
\
# Restore best L-BFGS state and record loss history\
if best["state"] is not None:\
    model.load_state_dict(best["state"])\
\
lbfgs_loss_history = inner_curve\
\
print(f"LBFGS finished. final_raw=\{final_raw:.4e\} | best_raw=\{best['loss']:.4e\} | inner_calls=\{len(inner_curve)\}")\
\
print(f"\\nTotal training time: \{end_training - start_training:.4f\} seconds."\
      f"\\nAdam: \{adam_training_finished - start_training:.4f\} seconds. "\
      f"L-BFGS: \{end_training - lbfgs_training_started:.4f\} seconds.")\
\
[\'85]\
Output:\
\
Using device: cuda\
[scales] sJ0=1.050e-03, sA=1.331e-01\
Starting Adam pre-training (SA-PINN with hard IC)...\
Adam Epoch 1000/25000 | Total=9.831e+00, PDE=9.831e+00, <pde_mask>=22.73 | Unmasked: Total=1.109e-02, PDE=1.109e-02 | lr_net=5.000e-03\
Adam Epoch 2000/25000 | Total=3.881e+01, PDE=3.881e+01, <pde_mask>=51.90 | Unmasked: Total=1.304e-02, PDE=1.304e-02 | lr_net=5.000e-03\
Adam Epoch 3000/25000 | Total=5.563e+01, PDE=5.563e+01, <pde_mask>=80.93 | Unmasked: Total=8.416e-03, PDE=8.416e-03 | lr_net=5.000e-03\
Adam Epoch 4000/25000 | Total=1.581e+02, PDE=1.581e+02, <pde_mask>=109.48 | Unmasked: Total=1.296e-02, PDE=1.296e-02 | lr_net=5.000e-03\
Adam Epoch 5000/25000 | Total=1.071e+02, PDE=1.071e+02, <pde_mask>=138.16 | Unmasked: Total=4.649e-03, PDE=4.649e-03 | lr_net=5.000e-03\
Adam Epoch 6000/25000 | Total=9.728e+01, PDE=9.728e+01, <pde_mask>=165.40 | Unmasked: Total=3.942e-03, PDE=3.942e-03 | lr_net=2.000e-03\
Adam Epoch 7000/25000 | Total=9.239e+01, PDE=9.239e+01, <pde_mask>=194.02 | Unmasked: Total=2.630e-03, PDE=2.630e-03 | lr_net=2.000e-03\
Adam Epoch 8000/25000 | Total=1.003e+02, PDE=1.003e+02, <pde_mask>=221.43 | Unmasked: Total=2.250e-03, PDE=2.250e-03 | lr_net=2.000e-03\
Adam Epoch 9000/25000 | Total=2.840e+01, PDE=2.840e+01, <pde_mask>=248.04 | Unmasked: Total=4.159e-04, PDE=4.159e-04 | lr_net=2.000e-03\
Adam Epoch 10000/25000 | Total=1.005e+01, PDE=1.005e+01, <pde_mask>=275.25 | Unmasked: Total=1.457e-04, PDE=1.457e-04 | lr_net=8.000e-04\
Adam Epoch 11000/25000 | Total=2.915e+00, PDE=2.915e+00, <pde_mask>=302.09 | Unmasked: Total=3.203e-05, PDE=3.203e-05 | lr_net=3.200e-04\
Adam Epoch 12000/25000 | Total=4.640e+00, PDE=4.640e+00, <pde_mask>=329.84 | Unmasked: Total=4.542e-05, PDE=4.542e-05 | lr_net=3.200e-04\
Adam Epoch 13000/25000 | Total=3.282e+00, PDE=3.282e+00, <pde_mask>=356.95 | Unmasked: Total=2.699e-05, PDE=2.699e-05 | lr_net=3.200e-04\
Adam Epoch 14000/25000 | Total=3.761e+00, PDE=3.761e+00, <pde_mask>=383.80 | Unmasked: Total=2.602e-05, PDE=2.602e-05 | lr_net=3.200e-04\
Adam Epoch 15000/25000 | Total=6.835e+00, PDE=6.835e+00, <pde_mask>=410.52 | Unmasked: Total=4.485e-05, PDE=4.485e-05 | lr_net=3.200e-04\
Adam Epoch 16000/25000 | Total=1.628e+00, PDE=1.628e+00, <pde_mask>=437.08 | Unmasked: Total=8.361e-06, PDE=8.361e-06 | lr_net=1.280e-04\
Adam Epoch 17000/25000 | Total=1.891e+00, PDE=1.891e+00, <pde_mask>=465.10 | Unmasked: Total=8.742e-06, PDE=8.742e-06 | lr_net=1.280e-04\
Adam Epoch 18000/25000 | Total=1.885e+00, PDE=1.885e+00, <pde_mask>=492.60 | Unmasked: Total=7.649e-06, PDE=7.649e-06 | lr_net=1.280e-04\
Adam Epoch 19000/25000 | Total=1.564e+00, PDE=1.564e+00, <pde_mask>=520.24 | Unmasked: Total=5.637e-06, PDE=5.637e-06 | lr_net=1.280e-04\
Adam Epoch 20000/25000 | Total=3.965e+00, PDE=3.965e+00, <pde_mask>=547.58 | Unmasked: Total=1.364e-05, PDE=1.364e-05 | lr_net=1.280e-04\
Adam Epoch 21000/25000 | Total=1.477e+00, PDE=1.477e+00, <pde_mask>=574.86 | Unmasked: Total=4.233e-06, PDE=4.233e-06 | lr_net=1.280e-04\
Adam Epoch 22000/25000 | Total=2.204e+00, PDE=2.204e+00, <pde_mask>=601.97 | Unmasked: Total=5.876e-06, PDE=5.876e-06 | lr_net=1.280e-04\
Adam Epoch 23000/25000 | Total=2.622e+00, PDE=2.622e+00, <pde_mask>=629.12 | Unmasked: Total=6.584e-06, PDE=6.584e-06 | lr_net=1.280e-04\
Adam Epoch 24000/25000 | Total=1.982e+00, PDE=1.982e+00, <pde_mask>=657.04 | Unmasked: Total=4.331e-06, PDE=4.331e-06 | lr_net=5.120e-05\
Adam Epoch 25000/25000 | Total=1.452e+00, PDE=1.452e+00, <pde_mask>=684.92 | Unmasked: Total=2.758e-06, PDE=2.758e-06 | lr_net=5.120e-05\
\
Adam finished. Best loss = 2.354e-06 | PDE=2.6942e-06\
LBFGS init unmasked PDE loss (from best Adam): 2.694e-06\
LBFGS finished. final_raw=2.6942e-06 | best_raw=1.5169e-06 | inner_calls=1000\
\
Total training time: 844.2951 seconds.\
Adam: 805.7448 seconds. L-BFGS: 38.5349 seconds.}