{\rtf1\ansi\ansicpg1252\cocoartf2821
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww18140\viewh12120\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 import time\
\
# Device configuration and core PyTorch setup\
import torch\
import torch.nn as nn\
import torch.nn.functional as F\
DTYPE = torch.float32\
device = torch.device('cuda')\
if device.type == "cuda":\
    torch.cuda.init()\
    torch.rand(1, device=device)\
print(f"Using device: \{device\}")\
\
from torch.optim.lr_scheduler import ReduceLROnPlateau\
import numpy as np\
import matplotlib.pyplot as plt\
from SA_PINN_ACTO import PINN_BDNK_1D\
from IC_1D import IC_BDNK\
\
# Hyperparameters for network architecture and training schedule\
Nl, Nn = 10, 70\
t_end = 10.0\
L = 50.0\
adam_epochs = 35_000\
lr_net = 5e-3\
lr_mask = 4e-2 \
\
# BDNK simulation configuration and background field setup\
from BDNK_Functions import *\
BDNK_simulation = 2\
setup_external_Tv(BDNK_simulation, L)\
\
# Sampling parameters and domain sampling\
N_colloc = 50000\
# In the ACTO case, the N_ic below are not collocation points where an IC residual will be computed,\
# but points at which the exact initial condition gets computed\
N_ic = 1000\
\
def lhs_box(n, low, high, rng=np.random):\
    low, high = np.asarray(low, float), np.asarray(high, float)\
    D = low.size\
    H = np.empty((n, D), float)\
    for j in range(D):\
        P = (rng.permutation(n) + rng.rand(n)) / n\
        H[:, j] = low[j] + P * (high[j] - low[j])\
    return H\
    \
X_colloc_np = lhs_box(N_colloc, low=np.array([0.0, -L]), high=np.array([t_end, L])).astype(np.float32)\
\
# Construction of initial condition sampling grid\
x_edges = np.linspace(-L, L, N_ic+1)\
x_ic = (0.5 * (x_edges[:-1] + x_edges[1:])).reshape(-1, 1)\
t_ic = np.zeros_like(x_ic)\
X_ic = np.hstack((t_ic, x_ic))\
\
X_ic_t = torch.tensor(X_ic, dtype=DTYPE, device=device)\
J0_ic_t, alpha_ic_t, _ = IC_BDNK(X_ic_t, L)\
\
# Scaling factors for numerical stability of IC enforcement\
with torch.no_grad():\
    sJ0 = J0_ic_t.abs().max().clamp_min(1e-12).item()\
    sA  = alpha_ic_t.abs().max().clamp_min(1e-12).item()\
print(f"[scales] sJ0=\{sJ0:.3e\}, sA=\{sA:.3e\}")\
\
# Sorting IC data to enable fast 1D interpolation\
x_ic_torch = X_ic_t[:, 1:2].contiguous().view(-1)\
x_sorted, idx_sort = torch.sort(x_ic_torch)\
J0_sorted    = J0_ic_t.view(-1)[idx_sort]\
alpha_sorted = alpha_ic_t.view(-1)[idx_sort]\
\
@torch.no_grad()\
def _torch_lin_interp_1d(xq: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\
    xq_flat = xq.view(-1)\
    xq_clamped = xq_flat.clamp(min=x[0], max=x[-1])\
    idx_hi = torch.searchsorted(x, xq_clamped, right=True)\
    idx_hi = idx_hi.clamp(min=1, max=x.numel() - 1)\
    idx_lo = idx_hi - 1\
    x0 = x[idx_lo]; x1 = x[idx_hi]\
    y0 = y[idx_lo]; y1 = y[idx_hi]\
    denom = (x1 - x0)\
    denom = torch.where(denom.abs() > 0, denom, torch.ones_like(denom))\
    w = (xq_clamped - x0) / denom\
    yq = y0 + w * (y1 - y0)\
    return yq.view_as(xq)\
\
# Initial condition functions passed to the neural network (physical scale)\
def J0_ic_func(x_phys: torch.Tensor) -> torch.Tensor:\
    if x_sorted.device != x_phys.device or x_sorted.dtype != x_phys.dtype:\
        xk = x_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\
        yk = J0_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\
    else:\
        xk, yk = x_sorted, J0_sorted\
    yq = _torch_lin_interp_1d(x_phys.view(-1), xk, yk)\
    return yq.view(-1, 1)\
\
def alpha_ic_func(x_phys: torch.Tensor) -> torch.Tensor:\
    if x_sorted.device != x_phys.device or x_sorted.dtype != x_phys.dtype:\
        xk = x_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\
        yk = alpha_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\
    else:\
        xk, yk = x_sorted, alpha_sorted\
    yq = _torch_lin_interp_1d(x_phys.view(-1), xk, yk)\
    return yq.view(-1, 1)\
\
# Scaled initial condition functions (used internally by the model)\
def J0_ic_func_scaled(x_phys: torch.Tensor) -> torch.Tensor:\
    return J0_ic_func(x_phys) / sJ0\
\
def alpha_ic_func_scaled(x_phys: torch.Tensor) -> torch.Tensor:\
    return alpha_ic_func(x_phys) / sA\
    \
X_colloc = torch.tensor(X_colloc_np, dtype=DTYPE, device=device)\
x0_line = torch.linspace(-L, L, 500, dtype=DTYPE, device=device).unsqueeze(1)\
X0 = torch.cat([torch.zeros_like(x0_line), x0_line], dim=1)\
X_colloc = torch.cat([X_colloc, X0], dim=0)\
\
# Self-adaptive collocation mask (learned weighting of PDE residual)\
pde_logits = torch.nn.Parameter(torch.zeros((X_colloc.shape[0], 1), dtype=DTYPE, device=device))\
def current_masks(detach: bool = False):\
    pde = F.softplus(pde_logits)\
    return pde.detach() if detach else pde\
\
# Model instantiation and domain normalization\
lb = torch.tensor([0.0, -L], dtype=DTYPE, device=device)\
ub = torch.tensor([t_end,  L], dtype=DTYPE, device=device)\
model = PINN_BDNK_1D(Nl=Nl, Nn=Nn, lb=lb, ub=ub).to(device).to(DTYPE)\
model.J0_ic_func    = J0_ic_func_scaled\
model.alpha_ic_func = alpha_ic_func_scaled\
model.sJ0.copy_(torch.tensor(sJ0, dtype=DTYPE, device=device))\
model.sA.copy_(torch.tensor(sA,  dtype=DTYPE, device=device))\
\
# Weight initialization\
def glorot_normal_all_linear(m):\
    if isinstance(m, nn.Linear):\
        nn.init.xavier_normal_(m.weight, gain=1.0)\
        if m.bias is not None: nn.init.zeros_(m.bias)\
model.apply(glorot_normal_all_linear)\
\
# Optimizers and learning-rate scheduler setup\
optimizer_theta = torch.optim.Adam(model.parameters(), lr=lr_net, betas=(0.9, 0.95))\
scheduler = ReduceLROnPlateau(optimizer_theta, mode='min', factor=0.4, patience=700, threshold=1e-4, min_lr=lr_net/100)\
optimizer_mask  = torch.optim.Adam([pde_logits], lr=lr_mask, betas=(0.7, 0.85), maximize=True)\
\
# Setting up and executing Adam pre-training\
def train_adam(model, optimizer_theta, optimizer_mask, epochs, print_every):\
    print("Starting Adam pre-training (SA-PINN with hard IC)...")\
    best_loss, best_state = float('inf'), None\
    loss_history = []\
\
    for epoch in range(1, epochs + 1):\
        optimizer_theta.zero_grad()\
        optimizer_mask.zero_grad()\
\
        R = model.pde_residual(X_colloc)\
        R1, R2 = R[:, 0:1], R[:, 1:2]\
        \
        Rnorm = torch.sqrt(R1**2 + R2**2)\
        pde_mask = current_masks(detach=False)\
        L_pde = (pde_mask * Rnorm).pow(2).mean()\
\
        loss = L_pde\
        if not torch.isfinite(loss): raise RuntimeError("Non-finite loss detected.")\
        \
        L_pde_phys   = (R1**2 + R2**2).mean()\
        L_total_phys = L_pde_phys\
\
        loss.backward()\
        optimizer_theta.step()\
        scheduler.step(L_total_phys.item())\
        optimizer_mask.step()\
\
        ltp = L_total_phys.detach().item()\
        loss_history.append(ltp)\
        if ltp < best_loss:\
            best_loss = ltp\
            best_state = \{k: v.detach().cpu().clone() for k, v in model.state_dict().items()\}\
\
        if epoch % print_every == 0 or epoch == epochs:\
            with torch.no_grad():\
                m_pde = current_masks(detach=True)\
            print(f"Adam Epoch \{epoch\}/\{epochs\} | "\
                  f"Total=\{loss:.3e\}, PDE=\{L_pde.item():.3e\}, <pde_mask>=\{m_pde.mean().item():.2f\} | "\
                  f"Unmasked: Total=\{L_total_phys:.3e\}, PDE=\{L_pde_phys.item():.3e\} | "\
                  f"lr_net=\{optimizer_theta.param_groups[0]['lr']:.3e\}")\
\
    if best_state is not None:\
        model.load_state_dict(best_state)\
        with torch.enable_grad():\
            R = model.pde_residual(X_colloc)\
            L_pde_phys = ((R[:,0:1]**2 + R[:,1:2]**2)).mean()\
        print(f"\\nAdam finished. Best loss = \{best_loss:.3e\} | PDE=\{L_pde_phys.item():.4e\}")\
    \
    return best_loss, loss_history, best_state\
    \
start_training = time.time()\
adam_loss, adam_loss_history, best_state = train_adam(model, optimizer_theta, optimizer_mask, adam_epochs, print_every=1000)\
adam_training_finished = time.time()\
\
model.load_state_dict(best_state)\
\
# Setting up executing L-BFGS fine-tuning\
with torch.enable_grad():\
    X_colloc.requires_grad_(True)\
    res = model.pde_residual(X_colloc)\
    init_lbfgs_loss = ((res[:,0:1]**2 + res[:,1:2]**2)).mean().detach().item()\
print(f"LBFGS init unmasked PDE loss (from best Adam): \{init_lbfgs_loss:.3e\}")\
\
loss_scale = 1.0 / max(init_lbfgs_loss, 1e-30)\
\
optimizer_lbfgs = torch.optim.LBFGS(\
    model.parameters(),\
    lr=1.0,\
    max_iter=1000,\
    max_eval=1000,\
    history_size=100,\
    line_search_fn=None,\
    tolerance_grad=1e-10,\
    tolerance_change=1e-12,\
)\
\
best = \{"loss": float("inf"), "state": None\}\
inner_curve = []\
\
def closure():\
    optimizer_lbfgs.zero_grad(set_to_none=True)\
    X = X_colloc.requires_grad_(True)\
\
    R = model.pde_residual(X)\
    R1, R2 = R[:, 0:1], R[:, 1:2]\
    raw = (R1.pow(2) + R2.pow(2)).mean()\
\
    if not torch.isfinite(raw):\
        print(f"NaN/Inf detected at iter \{len(inner_curve)\}. Exiting L-BFGS.")\
        raise RuntimeError("L-BFGS_NAN")\
\
    loss = raw * loss_scale\
\
    raw_f = float(raw)\
    if raw_f < best["loss"]:\
        best["loss"] = raw_f\
        best["state"] = \{k: v.detach().cpu().clone() for k, v in model.state_dict().items()\}\
\
    inner_curve.append(raw_f)\
\
    loss.backward()\
    return loss\
\
lbfgs_training_started = time.time()\
try:\
    final_loss = optimizer_lbfgs.step(closure)\
    final_raw = float(final_loss.item()) / loss_scale\
except RuntimeError as e:\
    if "L-BFGS_NAN" in str(e):\
        print("L-BFGS terminated early due to NaN/Inf.")\
        final_raw = float("nan")\
    else:\
        raise\
end_training = time.time()\
\
# Restore best L-BFGS state and record loss history\
if best["state"] is not None:\
    model.load_state_dict(best["state"])\
\
lbfgs_loss_history = inner_curve\
\
print(f"LBFGS finished. final_raw=\{final_raw:.4e\} | best_raw=\{best['loss']:.4e\} | inner_calls=\{len(inner_curve)\}")\
\
print(f"\\nTotal training time: \{end_training - start_training:.4f\} seconds."\
      f"\\nAdam: \{adam_training_finished - start_training:.4f\} seconds. "\
      f"L-BFGS: \{end_training - lbfgs_training_started:.4f\} seconds.")\
\
[\'85]\
Output:\
\
Using device: cuda\
[scales] sJ0=1.050e+00, sA=2.150e+01\
Starting Adam pre-training (SA-PINN with hard IC)...\
Adam Epoch 1000/35000 | Total=3.750e-03, PDE=3.750e-03, <pde_mask>=14.62 | Unmasked: Total=8.924e-06, PDE=8.924e-06 | lr_net=5.000e-03\
Adam Epoch 2000/35000 | Total=3.660e-02, PDE=3.660e-02, <pde_mask>=30.00 | Unmasked: Total=1.698e-05, PDE=1.698e-05 | lr_net=5.000e-03\
Adam Epoch 3000/35000 | Total=4.724e-02, PDE=4.724e-02, <pde_mask>=48.45 | Unmasked: Total=7.035e-06, PDE=7.035e-06 | lr_net=5.000e-03\
Adam Epoch 4000/35000 | Total=2.187e-01, PDE=2.187e-01, <pde_mask>=69.00 | Unmasked: Total=2.235e-05, PDE=2.235e-05 | lr_net=5.000e-03\
Adam Epoch 5000/35000 | Total=4.637e-02, PDE=4.637e-02, <pde_mask>=81.72 | Unmasked: Total=3.386e-06, PDE=3.386e-06 | lr_net=2.000e-03\
Adam Epoch 6000/35000 | Total=4.329e-02, PDE=4.329e-02, <pde_mask>=94.55 | Unmasked: Total=1.529e-06, PDE=1.529e-06 | lr_net=2.000e-03\
Adam Epoch 7000/35000 | Total=1.137e-01, PDE=1.137e-01, <pde_mask>=108.12 | Unmasked: Total=3.481e-06, PDE=3.481e-06 | lr_net=2.000e-03\
Adam Epoch 8000/35000 | Total=2.839e-02, PDE=2.839e-02, <pde_mask>=116.87 | Unmasked: Total=6.194e-07, PDE=6.194e-07 | lr_net=8.000e-04\
Adam Epoch 9000/35000 | Total=4.860e-02, PDE=4.860e-02, <pde_mask>=123.06 | Unmasked: Total=8.624e-07, PDE=8.624e-07 | lr_net=8.000e-04\
Adam Epoch 10000/35000 | Total=3.055e-02, PDE=3.055e-02, <pde_mask>=129.33 | Unmasked: Total=6.046e-07, PDE=6.046e-07 | lr_net=8.000e-04\
Adam Epoch 11000/35000 | Total=4.144e-02, PDE=4.144e-02, <pde_mask>=135.64 | Unmasked: Total=6.292e-07, PDE=6.292e-07 | lr_net=8.000e-04\
Adam Epoch 12000/35000 | Total=1.129e-02, PDE=1.129e-02, <pde_mask>=139.09 | Unmasked: Total=9.814e-08, PDE=9.814e-08 | lr_net=3.200e-04\
Adam Epoch 13000/35000 | Total=1.946e-02, PDE=1.946e-02, <pde_mask>=141.62 | Unmasked: Total=2.018e-07, PDE=2.018e-07 | lr_net=3.200e-04\
Adam Epoch 14000/35000 | Total=2.140e-02, PDE=2.140e-02, <pde_mask>=144.14 | Unmasked: Total=2.376e-07, PDE=2.376e-07 | lr_net=3.200e-04\
Adam Epoch 15000/35000 | Total=1.545e-02, PDE=1.545e-02, <pde_mask>=146.71 | Unmasked: Total=1.018e-07, PDE=1.018e-07 | lr_net=3.200e-04\
Adam Epoch 16000/35000 | Total=9.590e-03, PDE=9.590e-03, <pde_mask>=148.05 | Unmasked: Total=6.639e-08, PDE=6.639e-08 | lr_net=1.280e-04\
Adam Epoch 17000/35000 | Total=9.349e-03, PDE=9.349e-03, <pde_mask>=149.22 | Unmasked: Total=6.213e-08, PDE=6.213e-08 | lr_net=1.280e-04\
Adam Epoch 18000/35000 | Total=7.670e-03, PDE=7.670e-03, <pde_mask>=150.42 | Unmasked: Total=4.056e-08, PDE=4.056e-08 | lr_net=1.280e-04\
Adam Epoch 19000/35000 | Total=1.222e-02, PDE=1.222e-02, <pde_mask>=151.61 | Unmasked: Total=9.522e-08, PDE=9.522e-08 | lr_net=1.280e-04\
Adam Epoch 20000/35000 | Total=7.622e-03, PDE=7.622e-03, <pde_mask>=152.81 | Unmasked: Total=3.610e-08, PDE=3.610e-08 | lr_net=1.280e-04\
Adam Epoch 21000/35000 | Total=8.438e-03, PDE=8.438e-03, <pde_mask>=154.02 | Unmasked: Total=3.825e-08, PDE=3.825e-08 | lr_net=1.280e-04\
Adam Epoch 22000/35000 | Total=7.485e-03, PDE=7.485e-03, <pde_mask>=154.93 | Unmasked: Total=3.300e-08, PDE=3.300e-08 | lr_net=5.120e-05\
Adam Epoch 23000/35000 | Total=8.210e-03, PDE=8.210e-03, <pde_mask>=155.76 | Unmasked: Total=3.489e-08, PDE=3.489e-08 | lr_net=5.120e-05\
Adam Epoch 24000/35000 | Total=1.109e-02, PDE=1.109e-02, <pde_mask>=156.60 | Unmasked: Total=4.080e-08, PDE=4.080e-08 | lr_net=5.120e-05\
Adam Epoch 25000/35000 | Total=8.748e-03, PDE=8.748e-03, <pde_mask>=157.45 | Unmasked: Total=3.350e-08, PDE=3.350e-08 | lr_net=5.120e-05\
Adam Epoch 26000/35000 | Total=8.433e-03, PDE=8.433e-03, <pde_mask>=158.30 | Unmasked: Total=3.454e-08, PDE=3.454e-08 | lr_net=5.120e-05\
Adam Epoch 27000/35000 | Total=8.732e-03, PDE=8.732e-03, <pde_mask>=159.16 | Unmasked: Total=3.229e-08, PDE=3.229e-08 | lr_net=5.120e-05\
Adam Epoch 28000/35000 | Total=8.553e-03, PDE=8.553e-03, <pde_mask>=160.03 | Unmasked: Total=3.020e-08, PDE=3.020e-08 | lr_net=5.120e-05\
Adam Epoch 29000/35000 | Total=9.950e-03, PDE=9.950e-03, <pde_mask>=160.90 | Unmasked: Total=3.205e-08, PDE=3.205e-08 | lr_net=5.120e-05\
Adam Epoch 30000/35000 | Total=9.084e-03, PDE=9.084e-03, <pde_mask>=161.78 | Unmasked: Total=2.818e-08, PDE=2.818e-08 | lr_net=5.120e-05\
Adam Epoch 31000/35000 | Total=9.407e-03, PDE=9.407e-03, <pde_mask>=162.67 | Unmasked: Total=2.780e-08, PDE=2.780e-08 | lr_net=5.120e-05\
Adam Epoch 32000/35000 | Total=9.529e-03, PDE=9.529e-03, <pde_mask>=163.56 | Unmasked: Total=3.027e-08, PDE=3.027e-08 | lr_net=5.120e-05\
Adam Epoch 33000/35000 | Total=9.590e-03, PDE=9.590e-03, <pde_mask>=164.46 | Unmasked: Total=2.809e-08, PDE=2.809e-08 | lr_net=5.120e-05\
Adam Epoch 34000/35000 | Total=9.537e-03, PDE=9.537e-03, <pde_mask>=165.37 | Unmasked: Total=2.668e-08, PDE=2.668e-08 | lr_net=5.120e-05\
Adam Epoch 35000/35000 | Total=1.122e-02, PDE=1.122e-02, <pde_mask>=166.27 | Unmasked: Total=2.954e-08, PDE=2.954e-08 | lr_net=5.000e-05\
\
Adam finished. Best loss = 2.580e-08 | PDE=2.7879e-08\
LBFGS init unmasked PDE loss (from best Adam): 2.788e-08\
LBFGS finished. final_raw=2.7879e-08 | best_raw=2.2337e-08 | inner_calls=1000\
\
Total training time: 1030.2751 seconds.\
Adam: 995.4951 seconds. L-BFGS: 34.7704 seconds.}