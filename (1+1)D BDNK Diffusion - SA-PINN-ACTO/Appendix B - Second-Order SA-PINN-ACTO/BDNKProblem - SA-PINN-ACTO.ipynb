{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Device configuration and core PyTorch setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "DTYPE = torch.float32\n",
    "device = torch.device('cuda')\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.init()\n",
    "    torch.rand(1, device=device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from SA_PINN_ACTO import PINN_BDNK_1D\n",
    "from IC_1D import IC_BDNK\n",
    "\n",
    "# Hyperparameters for network architecture and training schedule\n",
    "Nl, Nn = 10, 70\n",
    "t_end = 20.0\n",
    "L = 50.0\n",
    "adam_epochs = 40_000\n",
    "lr_net = 2e-3\n",
    "lr_mask = 8e-2\n",
    "\n",
    "# Sampling parameters and domain sampling\n",
    "N_colloc = 20_000\n",
    "N_ic = 1000\n",
    "N_bc = 1000\n",
    "\n",
    "def lhs_box(n, low, high, rng=np.random):\n",
    "    low, high = np.asarray(low, float), np.asarray(high, float)\n",
    "    D = low.size\n",
    "    H = np.empty((n, D), float)\n",
    "    for j in range(D):\n",
    "        P = (rng.permutation(n) + rng.rand(n)) / n\n",
    "        H[:, j] = low[j] + P * (high[j] - low[j])\n",
    "    return H\n",
    "\n",
    "X_colloc_np = lhs_box(N_colloc, low=np.array([0.0, -L]), high=np.array([t_end, L])).astype(np.float32)\n",
    "\n",
    "# Construction of initial condition sampling grid\n",
    "x_edges = np.linspace(-L, L, N_ic+1)\n",
    "x_ic = (0.5 * (x_edges[:-1] + x_edges[1:])).reshape(-1, 1)\n",
    "t_ic = np.zeros_like(x_ic)\n",
    "X_ic = np.hstack((t_ic, x_ic))\n",
    "\n",
    "X_ic_t = torch.tensor(X_ic, dtype=DTYPE, device=device)\n",
    "alpha1st_ic_t, alpha2nd_ic_t = IC_BDNK(X_ic_t, L)\n",
    "\n",
    "# Scaling factors for numerical stability of IC enforcement\n",
    "with torch.no_grad():\n",
    "    sA = alpha1st_ic_t.abs().max().clamp_min(1e-12).item()\n",
    "print(f\"[scales] sA={sA:.3e}\")\n",
    "\n",
    "# Sorting IC data to enable fast 1D interpolation\n",
    "x_ic_torch = X_ic_t[:, 1:2].contiguous().view(-1)\n",
    "x_sorted, idx_sort = torch.sort(x_ic_torch)\n",
    "alpha1st_sorted = alpha1st_ic_t.view(-1)[idx_sort]\n",
    "alpha2nd_sorted = alpha2nd_ic_t.view(-1)[idx_sort]\n",
    "\n",
    "@torch.no_grad()\n",
    "def _torch_lin_interp_1d(xq: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    xq_flat = xq.view(-1)\n",
    "    xq_clamped = xq_flat.clamp(min=x[0], max=x[-1])\n",
    "    idx_hi = torch.searchsorted(x, xq_clamped, right=True)\n",
    "    idx_hi = idx_hi.clamp(min=1, max=x.numel() - 1)\n",
    "    idx_lo = idx_hi - 1\n",
    "    x0 = x[idx_lo]; x1 = x[idx_hi]\n",
    "    y0 = y[idx_lo]; y1 = y[idx_hi]\n",
    "    denom = (x1 - x0)\n",
    "    denom = torch.where(denom.abs() > 0, denom, torch.ones_like(denom))\n",
    "    w = (xq_clamped - x0) / denom\n",
    "    yq = y0 + w * (y1 - y0)\n",
    "    return yq.view_as(xq)\n",
    "\n",
    "# Initial condition functions passed to the neural network (physical scale)\n",
    "def alpha1st_ic_func(x_phys: torch.Tensor) -> torch.Tensor:\n",
    "    if x_sorted.device != x_phys.device or x_sorted.dtype != x_phys.dtype:\n",
    "        xk = x_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\n",
    "        yk = alpha1st_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\n",
    "    else:\n",
    "        xk, yk = x_sorted, alpha1st_sorted\n",
    "    yq = _torch_lin_interp_1d(x_phys.view(-1), xk, yk)\n",
    "    return yq.view(-1, 1)\n",
    "\n",
    "def alpha2nd_ic_func(x_phys: torch.Tensor) -> torch.Tensor:\n",
    "    if x_sorted.device != x_phys.device or x_sorted.dtype != x_phys.dtype:\n",
    "        xk = x_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\n",
    "        yk = alpha2nd_sorted.to(device=x_phys.device, dtype=x_phys.dtype)\n",
    "    else:\n",
    "        xk, yk = x_sorted, alpha2nd_sorted\n",
    "    yq = _torch_lin_interp_1d(x_phys.view(-1), xk, yk)\n",
    "    return yq.view(-1, 1)\n",
    "\n",
    "# Scaled initial condition functions (used internally by the model)\n",
    "def alpha1st_ic_func_scaled(x_phys: torch.Tensor) -> torch.Tensor:\n",
    "    return alpha1st_ic_func(x_phys) / sA\n",
    "\n",
    "def alpha2nd_ic_func_scaled(x_phys: torch.Tensor) -> torch.Tensor:\n",
    "    return alpha2nd_ic_func(x_phys) / sA\n",
    "\n",
    "xL = -L * np.ones((N_bc, 1))\n",
    "xR =  L * np.ones((N_bc, 1))\n",
    "t_bc = np.random.uniform(0.0, t_end, size=(N_bc, 1))\n",
    "X_bc_L = np.hstack((t_bc, xL))\n",
    "X_bc_R = np.hstack((t_bc, xR))\n",
    "\n",
    "X_colloc = torch.tensor(X_colloc_np, dtype=DTYPE, device=device)\n",
    "x0_line = torch.linspace(-L, L, 500, dtype=DTYPE, device=device).unsqueeze(1)\n",
    "X0 = torch.cat([torch.zeros_like(x0_line), x0_line], dim=1)\n",
    "X_colloc = torch.cat([X_colloc, X0], dim=0)\n",
    "\n",
    "X_bc_L = torch.tensor(X_bc_L, dtype=DTYPE, device=device)\n",
    "X_bc_R = torch.tensor(X_bc_R, dtype=DTYPE, device=device)\n",
    "\n",
    "Nt = 100\n",
    "x_mass = torch.tensor(x_ic.flatten(), dtype=DTYPE, device=device)\n",
    "t_mass = torch.linspace(0, t_end, Nt, dtype=DTYPE, device=device)\n",
    "\n",
    "# Self-adaptive collocation mask (learned weighting of PDE residual)\n",
    "pde_logits = torch.nn.Parameter(torch.zeros((X_colloc.shape[0], 1), dtype=DTYPE, device=device))\n",
    "def current_masks(detach: bool = False):\n",
    "    pde = F.softplus(pde_logits)\n",
    "    return pde.detach() if detach else pde\n",
    "\n",
    "# Model instantiation and domain normalization\n",
    "lb = torch.tensor([0.0, -L], dtype=DTYPE, device=device)\n",
    "ub = torch.tensor([t_end,  L], dtype=DTYPE, device=device)\n",
    "model = PINN_BDNK_1D(Nl=Nl, Nn=Nn, lb=lb, ub=ub).to(device).to(DTYPE)\n",
    "model.alpha_ic_func = alpha1st_ic_func_scaled\n",
    "model.sA.copy_(torch.tensor(sA, dtype=DTYPE, device=device))\n",
    "\n",
    "# Weight initialization\n",
    "def glorot_normal_all_linear(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight, gain=1.0)\n",
    "        if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "model.apply(glorot_normal_all_linear)\n",
    "\n",
    "# Optimizers and learning-rate scheduler setup\n",
    "optimizer_theta = torch.optim.Adam(model.parameters(), lr=lr_net, betas=(0.85, 0.92))\n",
    "scheduler = ReduceLROnPlateau(optimizer_theta, mode='min', factor=0.6, patience=2500, threshold=1e-4, min_lr=lr_net/100)\n",
    "optimizer_mask  = torch.optim.Adam([pde_logits], lr=lr_mask, betas=(0.7, 0.85), maximize=True)\n",
    "\n",
    "# Setting up and executing Adam pre-training\n",
    "def train_adam(model, optimizer_theta, optimizer_mask, epochs, print_every):\n",
    "    print(\"Starting Adam pre-training (SA-PINN with hard IC)...\")\n",
    "    best_loss, best_state = float('inf'), None\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        optimizer_theta.zero_grad()\n",
    "        optimizer_mask.zero_grad()\n",
    "\n",
    "        R = model.pde_residual(X_colloc)\n",
    "        Rnorm = R.abs()\n",
    "        pde_mask = current_masks(detach=False)\n",
    "        L_pde = (pde_mask * Rnorm).pow(2).mean()\n",
    "\n",
    "        L_ic = model.loss_ic(X_ic_t, alpha2nd_ic_t)\n",
    "        loss = L_pde + L_ic\n",
    "        if not torch.isfinite(loss): raise RuntimeError(\"Non-finite loss detected.\")\n",
    "\n",
    "        L_pde_phys   = (R**2).mean()\n",
    "        L_total_phys = L_pde_phys + L_ic\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_theta.step()\n",
    "        scheduler.step(L_total_phys.item())\n",
    "        optimizer_mask.step()\n",
    "\n",
    "        ltp = L_total_phys.detach().item()\n",
    "        loss_history.append(ltp)\n",
    "        if ltp < best_loss:\n",
    "            best_loss = ltp\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        if epoch % print_every == 0 or epoch == epochs:\n",
    "            with torch.no_grad():\n",
    "                m_pde = current_masks(detach=True)\n",
    "            print(f\"Adam Epoch {epoch}/{epochs} | \"\n",
    "                  f\"Total={loss:.3e}, PDE={L_pde.item():.3e}, <pde_mask>={m_pde.mean().item():.2f} | \"\n",
    "                  f\"Unmasked: Total={L_total_phys:.3e}, PDE={L_pde_phys.item():.3e}, IC={L_ic:.3e} | \"\n",
    "                  f\"lr_net={optimizer_theta.param_groups[0]['lr']:.3e}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        with torch.enable_grad():\n",
    "            R = model.pde_residual(X_colloc)\n",
    "            L_pde_phys = (R**2).mean()\n",
    "            L_ic = model.loss_ic(X_ic_t, alpha2nd_ic_t)\n",
    "        print(f\"\\nAdam finished. Best loss = {best_loss:.3e} | PDE={L_pde_phys.item():.4e}, IC={L_ic.item():.4e}\")\n",
    "\n",
    "    return best_loss, loss_history, best_state\n",
    "\n",
    "start_training = time.time()\n",
    "adam_loss, adam_loss_history, best_state = train_adam(model, optimizer_theta, optimizer_mask, adam_epochs, print_every=1000)\n",
    "adam_training_finished = time.time()\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "# Setting up executing L-BFGS fine-tuning\n",
    "with torch.enable_grad():\n",
    "    X_colloc_tmp = X_colloc.clone().detach().requires_grad_(True)\n",
    "    R0   = model.pde_residual(X_colloc_tmp)\n",
    "    L_IC = model.loss_ic(X_ic_t, alpha2nd_ic_t)\n",
    "    init_lbfgs_loss = (R0**2).mean().detach().item() + L_IC.detach().item()\n",
    "\n",
    "print(f\"LBFGS init unmasked PDE loss (from best Adam): {init_lbfgs_loss:.3e}\")\n",
    "\n",
    "loss_scale = 1.0 / max(init_lbfgs_loss, 1e-30)\n",
    "\n",
    "optimizer_lbfgs = torch.optim.LBFGS(\n",
    "    model.parameters(),\n",
    "    lr=1.0,\n",
    "    max_iter=3000,\n",
    "    max_eval=3000,\n",
    "    history_size=3000,\n",
    "    line_search_fn=None,\n",
    "    tolerance_grad=1e-10,\n",
    "    tolerance_change=1e-12,\n",
    ")\n",
    "\n",
    "best = {\"loss\": float(\"inf\"), \"state\": None}\n",
    "inner_curve = []\n",
    "\n",
    "def closure():\n",
    "    optimizer_lbfgs.zero_grad(set_to_none=True)\n",
    "    X = X_colloc.requires_grad_(True)\n",
    "\n",
    "    R = model.pde_residual(X)\n",
    "    L_ic = model.loss_ic(X_ic_t, alpha2nd_ic_t)\n",
    "    raw = (R.pow(2)).mean() + L_ic\n",
    "\n",
    "    if not torch.isfinite(raw):\n",
    "        print(f\"NaN/Inf detected at iter {len(inner_curve)}. Exiting L-BFGS.\")\n",
    "        raise RuntimeError(\"L-BFGS_NAN\")\n",
    "\n",
    "    loss = raw * loss_scale\n",
    "\n",
    "    raw_f = float(raw)\n",
    "    if raw_f < best[\"loss\"]:\n",
    "        best[\"loss\"] = raw_f\n",
    "        best[\"state\"] = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    inner_curve.append(raw_f)\n",
    "\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "lbfgs_training_started = time.time()\n",
    "try:\n",
    "    final_loss = optimizer_lbfgs.step(closure)\n",
    "    final_raw = float(final_loss.item()) / loss_scale\n",
    "except RuntimeError as e:\n",
    "    if \"L-BFGS_NAN\" in str(e):\n",
    "        print(\"L-BFGS terminated early due to NaN/Inf.\")\n",
    "        final_raw = float(\"nan\")\n",
    "    else:\n",
    "        raise\n",
    "end_training = time.time()\n",
    "\n",
    "# Restore best L-BFGS state and record loss history\n",
    "if best[\"state\"] is not None:\n",
    "    model.load_state_dict(best[\"state\"])\n",
    "\n",
    "with torch.enable_grad():\n",
    "    X_eval = X_colloc.clone().detach().requires_grad_(True)\n",
    "    R_eval = model.pde_residual(X_eval)\n",
    "    L_pde_best = (R_eval**2).mean().detach().item()\n",
    "    L_ic_best = model.loss_ic(X_ic_t, alpha2nd_ic_t).detach().item()\n",
    "    total_best = L_pde_best + L_ic_best\n",
    "\n",
    "lbfgs_loss_history = inner_curve\n",
    "\n",
    "print(f\"LBFGS finished. Final total loss={final_raw:.4e} | Best total loss={best['loss']:.4e} | \"\n",
    "      f\"PDE={L_pde_best:.4e} | IC={L_ic_best:.4e} | Iterations={len(inner_curve)}\")\n",
    "\n",
    "print(f\"\\nTotal training time: {end_training - start_training:.4f} seconds.\"\n",
    "      f\"\\nAdam: {adam_training_finished - start_training:.4f} seconds. \"\n",
    "      f\"L-BFGS: {end_training - lbfgs_training_started:.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5828a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import *\n",
    "\n",
    "plot_collocation_points(X_colloc, X_ic=None, X_bc_L=None, X_bc_R=None, L=L, t_end=t_end)\n",
    "\n",
    "alpha_ic_t = alpha1st_ic_t\n",
    "\n",
    "x_vals_ic = X_ic_t[:, 1:2]\n",
    "d, f, g = 0.05, 10.0, 1.05\n",
    "J0_ic_t = (d * torch.exp(- (f * x_vals_ic / L)**2) + g)\n",
    "\n",
    "J0_ic_np    = J0_ic_t.detach().cpu().numpy()\n",
    "alpha_ic_np = alpha_ic_t.detach().cpu().numpy()\n",
    "\n",
    "x_edges_eval = np.linspace(-L, L, 200+1)\n",
    "x_eval = 0.5 * (x_edges_eval[:-1] + x_edges_eval[1:])\n",
    "t_eval = np.linspace(0, t_end, 200)\n",
    "\n",
    "plot_results(\n",
    "    model,\n",
    "    t_eval=t_eval,\n",
    "    x_eval=x_eval,\n",
    "    alpha_ic=alpha_ic_t.detach().cpu().numpy(),\n",
    "    J0_ic=J0_ic_t.detach().cpu().numpy()\n",
    ")\n",
    "\n",
    "with torch.enable_grad():\n",
    "    plot_pde_residuals(model, t_eval, x_eval)\n",
    "\n",
    "lbfgs_history = {\"all_inner_per_epoch\": [lbfgs_loss_history]}\n",
    "plot_combined_loss_history(adam_loss_history, lbfgs_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d56a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "\n",
    "import os, time\n",
    "from typing import Dict, Any\n",
    "\n",
    "def _as_numpy(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy()\n",
    "    return np.asarray(x)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _eval_on_grid(model, t_eval: np.ndarray, x_eval: np.ndarray, batch_size: int = 32768):\n",
    "    \"\"\"\n",
    "    Evaluates:\n",
    "      - PDE residual R0 = model.pde_residual(...)\n",
    "      - α(t,x), J^0(t,x), n(t,x)\n",
    "    over t_eval × x_eval.\n",
    "    Returns a dict with all grids, to be dumped in the npz.\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    t_eval = np.asarray(t_eval, dtype=np.float32).ravel()\n",
    "    x_eval = np.asarray(x_eval, dtype=np.float32).ravel()\n",
    "    Nt, Nx = len(t_eval), len(x_eval)\n",
    "\n",
    "    TT, XX = np.meshgrid(t_eval, x_eval, indexing='ij')\n",
    "    TX = np.stack([TT.reshape(-1), XX.reshape(-1)], axis=1)\n",
    "    \n",
    "    TX_t = torch.tensor(TX, dtype=DTYPE, device=device, requires_grad=True)\n",
    "    r_list = []\n",
    "    with torch.enable_grad():\n",
    "        for i in range(0, TX_t.shape[0], batch_size):\n",
    "            r = model.pde_residual(TX_t[i:i+batch_size])\n",
    "            r_list.append(r)\n",
    "        r_all = torch.cat(r_list, dim=0)\n",
    "    r_pde_grid = r_all.view(Nt, Nx, -1).detach().cpu().numpy()\n",
    "\n",
    "    TX_t2 = torch.tensor(TX, dtype=DTYPE, device=device, requires_grad=True)\n",
    "    with torch.enable_grad():\n",
    "        alpha = model(TX_t2)\n",
    "\n",
    "        grad_alpha = torch.autograd.grad(\n",
    "            alpha, TX_t2,\n",
    "            grad_outputs=torch.ones_like(alpha),\n",
    "            create_graph=True, retain_graph=True\n",
    "        )[0]\n",
    "        alpha_t = grad_alpha[:, 0:1]\n",
    "\n",
    "        t_tensor = TX_t2[:, 0:1]\n",
    "        x_tensor = TX_t2[:, 1:2]\n",
    "        T_tensor = T_func(t_tensor, x_tensor)\n",
    "        v_tensor = v_func(t_tensor, x_tensor)\n",
    "\n",
    "        n_tensor  = n_from_alpha_func(alpha, T_tensor)\n",
    "        J0_tensor = J0_func(T_tensor, v_tensor, alpha, alpha_t, TX_t2)\n",
    "\n",
    "    alpha_grid = alpha.view(Nt, Nx, 1).detach().cpu().numpy()\n",
    "    J0_grid    = J0_tensor.view(Nt, Nx).detach().cpu().numpy().astype(np.float32)\n",
    "    n_grid     = n_tensor.view(Nt, Nx).detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    pred_grid  = alpha_grid\n",
    "    pred_names = np.array([\"alpha\"], dtype=object)\n",
    "\n",
    "    # restore mode\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return {\n",
    "        \"t_eval\": t_eval,\n",
    "        \"x_eval\": x_eval,\n",
    "        \"TX\": TX.astype(np.float32),\n",
    "        \"r_pde_grid\": r_pde_grid,\n",
    "        \"pred_grid\": pred_grid if pred_grid is not None else None,\n",
    "        \"pred_names\": np.array(pred_names, dtype=object) if pred_names is not None else None,\n",
    "        \"J0_grid\": J0_grid,\n",
    "        \"n_grid\":  n_grid,\n",
    "    }\n",
    "\n",
    "def _compute_residual_fields(model, t_eval: np.ndarray, x_eval: np.ndarray):\n",
    "    model.eval()\n",
    "\n",
    "    p = next(model.parameters())\n",
    "    dev, dty = p.device, p.dtype\n",
    "\n",
    "    t_eval = np.asarray(t_eval, dtype=np.float64).ravel()\n",
    "    x_eval = np.asarray(x_eval, dtype=np.float64).ravel()\n",
    "    Nt, Nx = len(t_eval), len(x_eval)\n",
    "\n",
    "    tt, xx = np.meshgrid(t_eval, x_eval, indexing='ij')\n",
    "    tx = np.column_stack([tt.ravel(), xx.ravel()])\n",
    "    tx_tensor = torch.tensor(tx, dtype=dty, device=dev, requires_grad=True)\n",
    "\n",
    "    def grad(u):\n",
    "        return torch.autograd.grad(\n",
    "            u, tx_tensor, grad_outputs=torch.ones_like(u),\n",
    "            create_graph=True, retain_graph=True\n",
    "        )[0]\n",
    "\n",
    "    with torch.set_grad_enabled(True):\n",
    "        alpha = model(tx_tensor)\n",
    "\n",
    "        t = tx_tensor[:, 0:1]\n",
    "        x = tx_tensor[:, 1:2]\n",
    "\n",
    "        T     = T_func(t, x)\n",
    "        v     = v_func(t, x)\n",
    "        gamma = gamma_func(v)\n",
    "\n",
    "        n     = n_from_alpha_func(alpha, T)\n",
    "        sigma = sigma_func(alpha, T)\n",
    "        lambd = lambd_func(sigma)\n",
    "\n",
    "        a_g     = grad(alpha)\n",
    "        alpha_t = a_g[:, 0:1]\n",
    "        alpha_x = a_g[:, 1:2]\n",
    "        N_x     = -alpha_x\n",
    "\n",
    "        J0 = J0_func(T, v, alpha, alpha_t, tx_tensor)\n",
    "        N_0 = N_0_func(lambd, sigma, T, J0, n, N_x, v)\n",
    "        Jx  = Jx_func(n, sigma, lambd, T, N_x, N_0, v)\n",
    "\n",
    "        # R1\n",
    "        J0_t = grad(J0)[:, 0:1]\n",
    "        Jx_x = grad(Jx)[:, 1:2]\n",
    "        R1   = J0_t + Jx_x\n",
    "\n",
    "        # R2\n",
    "        R2   = alpha_t + N_0\n",
    "\n",
    "        # R0\n",
    "        helper   = alpha_t + v * alpha_x\n",
    "        d_gn_dt  = grad(gamma * n)[:, 0:1]\n",
    "        d_gnv_dx = grad(gamma * n * v)[:, 1:2]\n",
    "        d_lt_dt  = grad((gamma**2) * lambd * T * helper)[:, 0:1]\n",
    "        d_lx_dx  = grad((gamma**2) * v * lambd * T * helper)[:, 1:2]\n",
    "        Wt       = -alpha_t + (gamma**2) * helper\n",
    "        Wx       =  alpha_x + (gamma**2) * v * helper\n",
    "        d_st_dt  = grad(sigma * T * Wt)[:, 0:1]\n",
    "        d_sx_dx  = grad(sigma * T * Wx)[:, 1:2]\n",
    "        R0 = d_gn_dt + d_gnv_dx + d_lt_dt + d_lx_dx - d_st_dt - d_sx_dx \n",
    "\n",
    "    def to_grid(Tv):\n",
    "        return Tv.detach().cpu().numpy().reshape(Nt, Nx)\n",
    "\n",
    "    return {\n",
    "        \"R1_grid\": to_grid(R1),\n",
    "        \"R2_grid\": to_grid(R2),\n",
    "        \"R0_grid\": to_grid(R0),\n",
    "    }\n",
    "\n",
    "res_fields = _compute_residual_fields(model, t_eval=t_eval, x_eval=x_eval)\n",
    "grid_dump = _eval_on_grid(model, t_eval=t_eval, x_eval=x_eval)\n",
    "run_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_dir = os.path.abspath(\"./pinn_runs\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "dump_path = os.path.join(save_dir, f\"PINN_run_dump_{run_id}.npz\")\n",
    "\n",
    "lbfgs_all_inner = np.array(lbfgs_history[\"all_inner_per_epoch\"], dtype=object) if \"all_inner_per_epoch\" in lbfgs_history else np.array([], dtype=object)\n",
    "lbfgs_inner_curve_np = np.asarray(lbfgs_loss_history, dtype=np.float64)\n",
    "\n",
    "np.savez_compressed(\n",
    "    dump_path,\n",
    "    run_id=run_id, dtype=str(DTYPE), device_type=str(device.type),\n",
    "    t_end=float(t_end), L=float(L),\n",
    "\n",
    "    X_colloc=_as_numpy(X_colloc),\n",
    "    X_bc_L=_as_numpy(X_bc_L) if 'X_bc_L' in globals() and X_bc_L is not None else np.array([]),\n",
    "    X_bc_R=_as_numpy(X_bc_R) if 'X_bc_R' in globals() and X_bc_R is not None else np.array([]),\n",
    "    x_ic=_as_numpy(x_ic), t_ic=_as_numpy(t_ic),\n",
    "    alpha_ic=_as_numpy(alpha_ic_np), J0_ic=_as_numpy(J0_ic_np),\n",
    "\n",
    "    t_eval=grid_dump[\"t_eval\"], x_eval=grid_dump[\"x_eval\"], TX=grid_dump[\"TX\"],\n",
    "    pred_grid=grid_dump[\"pred_grid\"] if grid_dump[\"pred_grid\"] is not None else np.array([]),\n",
    "    pred_names=grid_dump[\"pred_names\"] if grid_dump[\"pred_names\"] is not None else np.array([], dtype=object),\n",
    "    J0_grid=grid_dump[\"J0_grid\"],\n",
    "    n_grid=grid_dump[\"n_grid\"],\n",
    "    r_pde_grid=grid_dump[\"r_pde_grid\"],\n",
    "\n",
    "    R1_grid=res_fields[\"R1_grid\"],\n",
    "    R2_grid=res_fields[\"R2_grid\"],\n",
    "    R0_grid=res_fields[\"R0_grid\"],\n",
    "\n",
    "    adam_loss_history=np.asarray(adam_loss_history, dtype=np.float64),\n",
    "    lbfgs_final_per_epoch=np.asarray(lbfgs_history.get(\"final_per_epoch\", []), dtype=np.float64),\n",
    "    lbfgs_best_inner_per_epoch=np.asarray(lbfgs_history.get(\"best_inner_per_epoch\", []), dtype=np.float64),\n",
    "    lbfgs_num_closure_calls=np.asarray(lbfgs_history.get(\"num_closure_calls\", []), dtype=np.int32),\n",
    "    lbfgs_all_inner_per_epoch=lbfgs_all_inner,\n",
    "\n",
    "    lbfgs_inner_curve=lbfgs_inner_curve_np,\n",
    ")\n",
    "print(f\"[save] Dumped replot data to: {dump_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
